[
  {
    "objectID": "posts/2023-02-10_tidying-is-functional/index.html",
    "href": "posts/2023-02-10_tidying-is-functional/index.html",
    "title": "I used to think tidying was moral",
    "section": "",
    "text": "I’ve always struggled with tidying. Making my bed, gathering all the dirty clothes from the floor, loading the dishwasher and sanitizing the countertops—I struggle with all this to this day. But I feel better about it, and much more hopeful of finding a balance that works for me. Let me tell you why:\nI used to think tidying was a moral thing. Good people kept their spaces tidy, and the ones who didn’t were, by extension, bad people.\nIt was never spelled out like that. Instead, this dichotomy was enforced very effectively through shame and societal expectations. I felt ashamed of my messy spaces, and I felt like that shame was justified.1 Regardless of whether I wanted my room to be clean, it was clear that I should want it and that it should be clean. Jokes about Einstein having a messy desk were of limited help, because we all knew that in the real world people made their beds and why won’t you just do this one thing?2\nThe choices were few. I could ignore the issue, pretend I didn’t care. This was easy up to a point, because I often really don’t care. I will walk over piles of things to get to where I need to be and it won’t phase me in the least. I will make space just for my laptop on my messy desk and work just like that. But this state is never too long lasting, because things keep piling up, and with them so does the shame. So I pull out my next card, the “try” option. I try to organize. To clean. To put everything in its place and have a pristine space. Here I can either fail, only complete some partial cleaning, and feel bad; or I can succeed, maintain a clean state for a few hours or days, and then start to feel bad as it deteriorates again. Everyone wins!\nKC Davis changed the game. KC is the person behind StruggleCare.com, the TikTok channel DomesticBlisters, and much more. And this is what she has to say:\n\nMy first advice is to hear this sentence: Care tasks are morally neutral. They have nothing to do with being a good or bad person. We’re gonna stop thinking about whether something is good enough and we’re gonna start thinking about whether something functions in your home. And you are a person who deserves to function, even if you don’t like yourself.\n\n\n\nQuote from KC Davis on the Infinite Quest Podcast’s March 2022 episode entitled How to Keep House While Drowning With DomesticBlisters\nRight off the bat, she does away with the word “chores”. Chores are boring things that you need to do just because you’re expected to. Let’s forget about chores and focus on care tasks instead. Care tasks are entirely functional. You do them because doing them is required to make the spaces you inhabit function for you. You do them to care for yourself and your loved ones. And if doing them is not functional for you, if they don’t add anything you value to your life—you don’t need to do them!3\nAn example: If you have kids (and, let’s face it, even if you don’t), it can be difficult to have a pristine kitchen at any point. Dishes might pile up; counters may get soiled chaotically from little baking projects and snacks and spillages; fruit may be partially rotting somewhere; and when’s the last time you dusted in between that cupboard and the other thing? Well, perhaps the goal of completely cleaning your kitchen and keeping it that way forever is a tad unrealistic, dearie.\nPerhaps you just internalized that a good mother, husband, carer, however-you-identify should maintain a clean kitchen at all times, otherwise you are failing your family, your gender—heck—your nation4.\nPoppycock.\nWhat matters is if the kitchen functions for you and your household. Are important surfaces sanitized or potentially contaminated? Is there some space in the sink in case someone needs to wash something? Is fresh fruit visible and easy to access for those seeking a healthy snack? Forget what someone whose opinion is irrelevant might think you should be doing to your kitchen. What do you need for it to be functional?\nThis is step 0. The most important step comes next: approaching all this without judgment. Without assuming that the way you take care of your spaces defines your value as a person. Without feeling shame for liberating yourself from what is, in fact, not important to you. Without beating yourself up for not completing care tasks that are important, even though you meant to. Because if the whole point is caring, shaming yourself for failing is going to accomplish what, exactly? Instead, try to talk to yourself as you would to a dear friend in distress. Console yourself, remind yourself that you are doing what you can and it’s ok, consider reaching out for help—all those things that are easier said than done, and nevertheless worth practicing.\nOf course, figuring out what is functional for you and letting go of the shame is neither easy nor does it solve everything. If you still struggle to do tasks that are important to you, don’t despair. So do I. There are dozens of us, DOZENS!5 Perhaps one of KC’s many videos will be helpful to you, or you can consult her very accessible book. Oh, look, a resources list right below. How convenient!\n\nResources\n\nListen to KC on Infinite Quest (Spotify link): How to Keep House While Drowning With DomesticBlisters\nWatch KC on TikTok: DomesticBlisters\nFind similar content on her Instagram: strugglecare\nRead KC’s book: How to Keep House While Drowning: A Gentle Approach to Cleaning and Organizing\nCheck out KC’s own Resources Page on her website\n\n\n\nBonus\nYou’re still here? I’m flattered.\nFor real, though, I believe this concept has the potential to be extended to other areas of life where you might be holding on to unnecessary and dysfunctional expectations. Think of all the things you believe you’re supposed to do in order to lead a better, purer life. Perhaps they include such classics as Wake Up Earlier, Read More Fiction, or Eat Fewer Chips. Many people try to tell you those are noble pursuits to be ignored at one’s own risk. Well, those people don’t know what’s good for you. And, to be frank, they probably don’t know what’s good for themselves either. Don’t take their advice; eject it from your mind. Then fill the space back with things that you actually have good reason to believe would make you happier. Maybe they’re even the same things! But they’ll taste different this way :)\n\n\n\n\n\nFootnotes\n\n\nLongtime fans will recognize this as a form of introjected regulation, a kind of external regulation within Self-Determination Theory that I described in a past life.↩︎\nOf course, it’s never just one thing, but that’s a minor detail.↩︎\nPro-tip from Catieosaurus: If you hate folding laundry. If you just can’t stand it. Maybe consider… not. (Apply generously over all infected areas.)↩︎\n*gasp*↩︎\nThis is a reference to Arrested Development, not a trustworthy figure. I’d count on there being billions of us, actually. Those of you who’ve reached care task–Nirvana, I salute you. Now get out. Or help me?↩︎\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{brazão2023,\n  author = {Vasco Brazão},\n  title = {I Used to Think Tidying Was Moral},\n  date = {2023-02-10},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVasco Brazão. 2023. “I Used to Think Tidying Was Moral.”\nFebruary 10, 2023."
  },
  {
    "objectID": "posts/2023-06-09_devaneios-pos-twitter/index.html",
    "href": "posts/2023-06-09_devaneios-pos-twitter/index.html",
    "title": "Devaneios pós twitter e pré aula de ucraniano",
    "section": "",
    "text": "será que aquela portuguesa continua a aprender russo\nserá que aquele brasileiro sonha com a nato\n—e acorda a suar\nserá que apagam as notícias porque já não conseguem ver\n—ou já não querem saber\na comida afoga-se no estômago\nsó não volta cá pra fora porque empurro tudo pra dentro\nповітря вдихається uma ova\nsem fôlego, sem ar, esmagado, a flutuar\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{brazão2023,\n  author = {Vasco Brazão},\n  title = {Devaneios Pós Twitter e Pré Aula de Ucraniano},\n  date = {2023-06-08},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVasco Brazão. 2023. “Devaneios Pós Twitter e Pré Aula de\nUcraniano.” June 8, 2023."
  },
  {
    "objectID": "posts/2023-03-10_antifatness/index.html",
    "href": "posts/2023-03-10_antifatness/index.html",
    "title": "Anti-fat bias, not ‘obesity’, is a problem worth fighting",
    "section": "",
    "text": "I used to think fat bodies were inherently less healthy than thin bodies. I believed that fighting “obesity” at a societal and individual level was important. I also felt superior to fat people for having a thin body. Now, this makes me feel ashamed.\nMy mind was changed by listening to fat people1 who have done extensive research, and then going into the literature myself for hours. What I found was deeply troubling and compelled me to share it with you. Perhaps this will help you, as it has helped me, overcome the anti-fatness we’ve inherited and start working to dismantle it. If you don’t care about some thin guy’s thoughts on anti-fatness and “obesity” research, consider skipping straight to the Resources section and use the suggestions as a springboard for further education."
  },
  {
    "objectID": "posts/2023-03-10_antifatness/index.html#myth-1-it-is-proven-that-having-a-fat-body-is-inherently-unhealthy.",
    "href": "posts/2023-03-10_antifatness/index.html#myth-1-it-is-proven-that-having-a-fat-body-is-inherently-unhealthy.",
    "title": "Anti-fat bias, not ‘obesity’, is a problem worth fighting",
    "section": "Myth 1: It is proven that having a fat body is inherently unhealthy.",
    "text": "Myth 1: It is proven that having a fat body is inherently unhealthy.\nThis is simply imagined. There is no consensus at all in the literature, even though they’ve been trying for decades. Large studies find a protection against mortality for those who are slightly “overweight”, and a rise in mortality for those who are very thin or very fat2. Others say “no, that can’t be!!” and adjust analyses until they find what they are looking for. Check this, from a recent critique of meta-analyses of cohort studies3, emphasis mine:\n\nKarahalios and colleagues’ meta-analysis is the first to show that weight loss and weight gain in midlife are associated with increased risk of all-cause and cardiovascular disease mortality, and weakly associated with cancer mortality. They discussed weight loss intention, effect modification, reverse causation, bias and confounding, and stated that ‘this is the highest level of evidence possible’ and ‘these observational data suggest weight stability from middle age [is beneficial], however, further research investigating effect modification by obesity status is warranted.’\nAlthough the authors did not recommend weight stability regardless of health and weight status, the results based on observational data could be easily misinterpreted, with adverse public health implications. Our interpretation is that their result that weight gain showed higher risk than weight stability is plausible and confirms that increased adiposity is harmful. Such increased risk can be more easily observed when obesity has been rapidly increasing globally during the follow-up period of the cohorts, and we would expect more unhealthy weight gain and related harms than healthy weight reduction and related benefits in many cohort studies. However, their finding that weight reduction is associated with high mortality risk is unexpected and more problematic. It is most likely due to reverse causation, as the adverse effects of unintentional weight loss due to ill health and ageing in many participants would overwhelm any benefits of weight loss in the few attempting it intentionally.\n\nDid you catch it? This is such a common pattern in the literature that I’m honestly angry. Any evidence that weight gain is harmful is obvious and good; any evidence that weight loss is problematic is probably hopelessly confounded by many factors the researchers didn’t consider, and also too few people attempt to lose weight anyway (are you shitting me?). Take a look in the fucking research mirror.\nBut, OK, let’s imagine (it’s easy, doctors do it!) that there is a robust association between higher weight and higher risk of disease. You cannot then conclude that fat itself causes worse health outcomes, especially given the very strong evidence we have for a significant confounding factor: anti-fatness. Anti-fatness causes stress (from strangers giving you looks, telling you that your body is wrong, policing your food choices since childhood); it causes attempts to diet, which often result in losing weight, gaining it back, and trying to lose it again (called weight cycling or “yo-yo dieting”); it causes decreased access to medical care, because doctors refuse treatment or are so judgemental that you postpone a visit despite symptoms. Sounds important; doesn’t get measured. \nAnother obvious confounder is behavior. We know that getting regular exercise, eating vegetables, and many other behaviors can have a very positive impact on your health. Could it be that the heavier people in your sample also show fewer of these positive behaviors, and it’s the behaviors that are driving the difference in health outcomes? Without ruling out confounders, it’s irresponsible and unethical to extrapolate that recommending weight loss is a good thing. The thing is, you barely see studies applying this level of skepticism and looking for other causes when they document an association between fatness and health, but you do see those same studies saying that we should keep recommending weight loss.\nTo make it crystal clear: If you find data showing that tall people have higher salaries than shorter people in the same white-collar jobs, do you immediately conclude that height causes intelligence and job performance? If you see a racial minority perform worse in school than the non-minoritized children, do you make strong inferences about race-based differences in intelligence? No.\nAnd yet, “obesity” researchers and health care providers repeatedly do this when it comes to weight and health, ignoring alternative explanations in favor of a dogmatic focus on weight loss. This should be reason enough to be suspicious of the whole thing. Especially since there are bagillions of dollars to be made in convincing people that weight loss is necessary.\nBut let’s move on. Imagine that it were proven, beyond the shadow of a doubt, that higher amounts of fat lead to worse health outcomes. Imagine (again, the healthcare system does this all the time, you can too!) that we have super solid evidence that it is the fat itself that causes increased risk of certain diseases (reminder: we definitely do not). Then what?\nTell people to lose weight, right?"
  },
  {
    "objectID": "posts/2023-03-10_antifatness/index.html#myth-2-people-can-permanently-lose-a-significant-amount-of-weight-and-keep-it-off-if-they-want-to.",
    "href": "posts/2023-03-10_antifatness/index.html#myth-2-people-can-permanently-lose-a-significant-amount-of-weight-and-keep-it-off-if-they-want-to.",
    "title": "Anti-fat bias, not ‘obesity’, is a problem worth fighting",
    "section": "Myth 2: People can permanently lose a significant amount of weight and keep it off if they want to.",
    "text": "Myth 2: People can permanently lose a significant amount of weight and keep it off if they want to.\nThis is the most outrageous, because there are so. many. studies on this.\nDoctors and pharma companies keep trying to prove that one diet or another, one “treatment” or another, can lead to sustained weight loss, and they just can’t. The proportion of people who manage to lose weight and keep it off for over two years is eclipsed by the number of people who end up at the same weight or higher. Within five years, it’s basically zero. All these people were subjected to health harms (fast weight loss, weight cycling), potentially developing disordered eating or an eating disorder as a result, and for what?\nBy the way, these are MEDICALLY SUPERVISED DIETS. Randomized studies, where some people received a diet and others didn’t; or some received a drug and others a placebo. This is not based on people randomly going “hmm, guess I’ll try intermittent fasting” and switching back to their normal patterns after a few weeks. But we do kind of have data on that, too!\nOne study took data from around 170,000 “obese” people and followed them for several years. They received no intervention for this study, they were just observed to see how many achieved a weight considered “normal” for their height. Of those classified as “Class 1 obese”, the annual probability of achieving a “normal” weight was 1 in 210 for men and 1 in 124 for women. 0.4 and 0.8 percent, respectively. And for those in the “Class 3 obesity” category: 1 in 1,290 for men and 1 in 677 for women. Do you realize how low that is? Do you believe this is because none of those people wanted to and tried (probably multiple times) to lose weight? Do you think no doctor ever told them “you just need to lose some weight” when they came in with unrelated complaints? That no friend ever suggested that they order the salad instead of whatever they ordered?"
  },
  {
    "objectID": "posts/2023-03-10_antifatness/index.html#myth-3-when-you-have-a-fat-body-and-lose-a-significant-amount-of-weight-there-is-a-clear-benefit-to-your-health.",
    "href": "posts/2023-03-10_antifatness/index.html#myth-3-when-you-have-a-fat-body-and-lose-a-significant-amount-of-weight-there-is-a-clear-benefit-to-your-health.",
    "title": "Anti-fat bias, not ‘obesity’, is a problem worth fighting",
    "section": "Myth 3: When you have a fat body and lose a significant amount of weight, there is a clear benefit to your health.",
    "text": "Myth 3: When you have a fat body and lose a significant amount of weight, there is a clear benefit to your health.\nThis would be easier to study if it were possible to get many people to lose a significant amount of weight and keep it off, of course. Then we could know if it actually had a positive health impact, independent of health markers such as the lipid profile. But we can’t, so we don’t really know.4\nAnd yet we act as if this is simply established fact. Of COURSE you should diet and try to lose weight if you’re above your “ideal weight”. Even if you then gain the weight back, who cares! It was good for you to have it off for a couple of months. Ugghhhhh.\nAnd it gets worse! I have tried to find studies that show weight loss maintenance after 2 years, and the literature seems pretty sure that it generally doesn’t happen. However, it is true that some people manage to lose weight before gaining it back. When this happens, sometimes researchers try to test whether this weight loss led to some meaningful outcome (e.g., fewer people becoming diabetic) despite the fact that people regain the weight eventually. If so, jackpot! No need to test for the obvious confounder—whether people ate better and exercised more in this period and whether THAT is associated with a protective effect—since we already showed what we came here for (any weight loss, even if short lived, protects you against disease!). It’s almost as if there are enormous amounts of money to be made from people pursuing weight loss all their lives… but we don’t want to be conspiracy theorists. Of course it could also be due to medical researchers’ profound lack of statistical and causal inference education. But we don’t want to go around proposing obvious confounders unnecessarily, right?\nFUUUUCK. MEE.\n\n\n\n\n\n\nBonus myth: You can judge a person's health by looking at their current size.\n\n\n\n\n\nNo. The fat person you are judging could be at the peak of their health. The thin person you are congratulating could have just lost weight due to depression. The friend who gained a few kilos recently may finally be progressing on their mental health treatment and started taking antidepressants as prescribed. The friend who lost weight when they started exercising will probably gain it back, even if they keep exercising—yet they're still healthier than if they didn't exercise. The fat person you are congratulating for slimming down could be showing signs of an eating disorder that no one picks up on because we assume that any weight a fat person loses was for the best, even if they stopped menstruating months ago."
  },
  {
    "objectID": "posts/2023-03-10_antifatness/index.html#lay-a-foundation-for-change",
    "href": "posts/2023-03-10_antifatness/index.html#lay-a-foundation-for-change",
    "title": "Anti-fat bias, not ‘obesity’, is a problem worth fighting",
    "section": "Lay a foundation for change",
    "text": "Lay a foundation for change\n\nReflect on your life. Have you ever gently suggested your friend “just needs to lose some weight”? Have you ever commented on someone’s food choices, shamed them for not eating enough salad or whatever you thought was best for them, because they looked too fat to you? Have you ever suggested a specific diet to someone based on their looks? Then, unfortunately, you’ve done a horrible thing. Truly. You have contributed to someone’s internalized anti-fatness, have raised their chances of developing disordered eating, have perpetuated harmful lies that allow society to keep oppressing fat people. If you feel resistance at these statements, I get it. It sucks. Sit with the guilt. Sit with the shame. I’ll sit with you. We can cry, too, it’s alright. Let the reflections and the emotions till the soil of your mind so you’re ready to change. And then…\nTake the time to listen. Listen to one podcast episode about fatness BY A FAT PERSON; read one article about fatness BY A FAT PERSON; buy and read one of the several books Aubrey Gordon has written or recommends; follow one fat activist on Twitter, Instagram, TikTok, whatever. And if you find yourself having quick, knee-jerk reactions when they explain things to you, remember to shut up and I mean take the time to listen. Then go process things for a month or two. It will take time.\nLearn more, go on a research bender! Google “fat liberation” and spend some hours understanding the movement better. Go to Google Scholar and dig into the garbage research yourself. Find out what the Health At Every Size movement has been up to. Or use the more structured Resources list below.\nGet angry. Or don’t. What I mean to say is: anger is an appropriate reaction to this injustice. Once you pull back the curtain and see what’s underneath the “fight against ‘obesity’”, once you see the harm it causes to everyone, once you see yourself, your loved ones, your enemies through this new lens—well I’d be surprised if you didn’t feel at least a little bit angry. I’m fucking pissed."
  },
  {
    "objectID": "posts/2023-03-10_antifatness/index.html#beyond-awarenesshelp-stem-the-tide-of-anti-fatness",
    "href": "posts/2023-03-10_antifatness/index.html#beyond-awarenesshelp-stem-the-tide-of-anti-fatness",
    "title": "Anti-fat bias, not ‘obesity’, is a problem worth fighting",
    "section": "Beyond awareness—help stem the tide of anti-fatness",
    "text": "Beyond awareness—help stem the tide of anti-fatness\n\nStop congratulating weight loss. It’s so pervasive, so unconscious, we don’t even consider it, but spontaneously congratulating someone for looking thinner is perpetuating harm. This harm comes in many forms, but let me quote you something that really touched me recently7:\n\n\nThe thing for me, in terms of my weight and my mental health, is that, when I’ve been smallest (…) it’s usually when I’ve been the most unwell, and ironically when everyone’s been the most happy and pleased with me. (…) I remember one time (…) a vicar, actually, came up to me and he said, “Tanya you look marvelous! Tell me, what is your secret?” And I said, “Severe depression and staying in bed for two weeks.”\n\n\nStop encouraging weight loss. You’re not a doctor—it’s enough (too much, really) that they already deny patients care and humanity because of anti-fatness. Don’t spontaneously suggest other people should lose weight, or encourage them to go on this or that diet.\nStop supporting the “fight against obesity”. For example, at work: remove weight loss as an explicit goal of wellness programs (if they must exist at all); challenge your organization if you ever start projects with the explicit goal of “fighting obesity”.\nStop equating morality to body size. EVEN IF people were fat only because they lack the willpower to eat healthy and exercise (which is categorically not true), that has nothing to do with whether they are a good person, deserving of love and compassion. Do you consider it immoral to be depressed and not be able to get out of bed? To try and fail at quitting smoking? To need accommodations for being too short or too tall? Do you think it’s useful to shame any of these groups of people for how they are, in the hopes that they will change?\nStop equating health to body size, and start actually caring about the health of your loved ones. To participate in their health, ask: How are you feeling? Are you stressed? Do you sleep well? Do you manage to move, at least a bit each day? Do you manage to eat fruits and vegetables every day? If not, can I help you with any of these things? Do you have pain that you’re finding hard to manage? OK, maybe not everything at once, chill out.\nStart realizing that fast weight loss is a cause for concern. Let me quote you from The Fat Doctor Podcast again, to drive this point home. Dr Asher Larmie8:\n\n\nAnd during that time, mum was (...) going to salsa classes and enjoying herself and having the time of her life and she was so happy because she was finally losing weight. And she was just, she was rejoicing about it (…) Do you remember that time when she was like, “Girls, girls, look!” And she pulled her trousers down and she didn’t have to unbutton her jeans anymore, they just slid down because she had lost so much weight around her bottom. I remember very distinctly that she had mentioned around Christmas time (...) that she was getting some stomach pains and, you know, “that’s really weird isn’t it?” And I said, “Oh it is a bit weird, mum, why don’t you try some indigestion medication and if it doesn’t get any better, you know, let me know,” and then stuff happened (...) And then we got to February and I came over with the kids and (…) I walked into the house, Junior was with me (...) and mum didn’t look like herself, and I couldn’t quite put my finger on it, and all of a sudden, Junior sort of pulled me to one side and said, “Babes, look at your mum.” And I said, “What about her?” And he said, “Look at you mum, she’s jaundiced! She’s yellow!” And then I looked over at mum and I suddenly thought, “Oh. Yeah. She’s yellow. That’s not good.” And that was mid February, and by March we’d had the diagnosis of pancreatic cancer, and she died in July, and I… I guess she was so happy about the fact that she lost weight, that her own daughter, a GP, a doctor, (...) did not put two and two together and think “weight loss and abdominal pain in a woman in her 60s is never a good thing, she NEEDS to see a doctor”.\n\n\nStart looking out for fat oppression around you, and considering ways that you personally could help end it. The resources below will help with this. One last (promise!) inspirational quote for you:\n\n\nSometimes, too, our power may not be clear to us. Each of us may forget the influence that we have over those we know and love. We may also forget that we’ve developed a sharp skill set for shifting the thinking and actions of those closest to us. Those working or learning in schools may not realize they can request more accessible seating options for fat and disabled students and faculty. Those in workplaces with human resources departments can advocate for the end of anti-fat “workplace wellness” programs, or workplace “biggest loser” weight-loss competitions. Health-care providers and administrators can make sure that medical equipment like blood pressure cuffs and exam tables are built for fat bodies, and start difficult, vital conversations about tackling anti-fat bias in health care. And nearly all of us can do more to interrupt vicious anti-fatness when it rears its head in public. None of these actions in and of themselves will end anti-fatness, but they can help stem the tide of anti-fatness that fat people contend with every day. Take these opportunities for action as footholds, options for next steps in your work to support fat people[…].9\n\nI used to think we should fight “obesity”. Maybe you did too. I am now convinced that we’ll all be better off if we focus on health, not weight, and on fighting anti-fatness, not “obesity”. And you?"
  },
  {
    "objectID": "posts/2023-02-25_asd-norm/index.html",
    "href": "posts/2023-02-25_asd-norm/index.html",
    "title": "What if Autism was the norm?",
    "section": "",
    "text": "This post is an adaptation / archive of something I first wrote for Instagram (link to original post). The images are reposted in order with respective image descriptions.\nMore on the topic “What if neurodivergence was the norm?” Let’s subvert how we tend to think about Autism! If 95% of people were Autistic, how would we describe those with “Neurotypical Spectrum Disorder”?\n\nContent highly inspired by the work of @myfavouritejo on TikTok (also on Instagram @myfavouritejo).\n\n\n\nImage text:\nHow to spot a neurotypical\n5 signs of Neurotypical Spectrum Disorder\n(POV: ASD* is the norm)\n*Autism Spectrum Disorder\n\n\n\n\n\nImage text:\n1. Communication difficulties\nStrong irrational preference for indirect and ambiguous communication, even in the workplace. Asking them questions to clarify what they meant can often result in irritation or anger (in particular when they are in positions of power, think teachers or bosses). It is difficult to deal with these traits because they interact: their lack of clarity leads normal people to have questions, but we must be careful when asking so as not to hurt their feelings.\n\n\n\n\n\nImage text:\n2. Lack of empathy\nPeople with NSD struggle to understand others’ minds and emotions, which makes their inclusion in the world particularly challenging. Specifically, they have a deficit of affective empathy, not instinctively feeling the emotions of those around them, leading to difficulties interacting and forming bonds.\nNUANCE: neurotypicals seem to not have empathy challenges among each other, only exhibiting deficits when interacting with normal people, but more research is needed on the matter.\n\n\n\n\n\nImage text:\n3. Difficulty socializing\nStrong preference for white lies (“Fine, thanks, and you?”), chit chat devoid of any depth, and extreme amounts of eye contact. Strong levels of discomfort when they don’t receive intense eye contact (it seems like eye contact is soothing for people with NSD).\nNUANCE: neurotypicals seem to socialize just fine among each other, exhibiting deficits only when they socialize with normal people.\n\n\n\n\n\nImage text:\n4. Chronic imprecision\nNeurotypicals’ attention to detail is abysmal, which affects all aspects of their lives. They have difficulty learning basic rules like how best to organize books on a shelf or which spoon is correct for each food, but their professional life is affected as well, resulting, for instance, in PowerPoint presentations full of obviously misaligned elements. This may also explain their tenuous relationship with the truth and tremendous difficulty specifying and following rules.\n\n\n\n\n\nImage text:\n5. Weak sense of justice\nNormal nervous systems are strongly affected by injustice. People with NSD have an underdeveloped sense of justice and morality, perhaps due to their empathy deficits. They seem to lack the normal reaction to witnessing embarrassment, not showing any signs of physical pain or an anxious emotional state when watching scenes in which a character is humiliated, for example. At times, the lack of empathy is such that they laugh instead of feeling the appropriate emotions. How would society function if we were all so indifferent?\n\n\n\n\n\nImage text:\nAutistic people live in a world designed by and for neurotypical people.\n\nTo navigate the world, they must learn how others act and pretend to be like them.\n\nWhat if neurotypicals learned more about autism? What if we created a world that accommodates neurodiversity?\n\n\nColor palette inspired by Pedro Almodóvar’s “Women on the Verge of a Nervous Breakdown”.\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{brazão2023,\n  author = {Vasco Brazão},\n  title = {What If {Autism} Was the Norm?},\n  date = {2023-02-25},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVasco Brazão. 2023. “What If Autism Was the Norm?” February\n25, 2023."
  },
  {
    "objectID": "posts/2023-09-26_medium-friend-links/index.html",
    "href": "posts/2023-09-26_medium-friend-links/index.html",
    "title": "Medium Friend Links",
    "section": "",
    "text": "It seems I’ve taken to writing on Medium again. Find my account here: @vascobrazao\nThe possibility of earning some money with my writing is alluring, so I will paywall most, if not all, of my posts. You can read them for free by using the following links, which Medium calls “friend links”:\n\n2023-09-28: Ideologia de Género | Gender Ideology (BOUNCIN’ AND BEHAVIN’ INTERNATIONAL POETRY | PORTUGUESE)\n2023-09-27: Back from the dread | A poem\n2023-09-24: How to Get Rich Quick (with Quips) | Two poems, zero tips (POEMS | FREEVERSE | LIMERICK)\n2023-09-19: The Worst is Wrong and Boring | An eye roll at tedious bigots (POETRY | FREEVERSE)\n2023-09-16: My Natural Rhythm | Making sense of a busy mind (POETRY | FREEVERSE)\n2023-06-08: Devaneios pós twitter e pré aula de ucraniano | —um poema\n2023-03-10: Anti-fat bias, not ‘obesity’, is a problem worth fighting | Here’s what changed my mind\n2023-02-10: I used to think tidying was moral | It’s not. It’s functional. Let KC Davis tell you what that means.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{brazão2023,\n  author = {Vasco Brazão},\n  title = {Medium {Friend} {Links}},\n  date = {2023-09-26},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVasco Brazão. 2023. “Medium Friend Links.” September 26,\n2023."
  },
  {
    "objectID": "posts/2023-12-21_mediation-weighted-glm/index.html",
    "href": "posts/2023-12-21_mediation-weighted-glm/index.html",
    "title": "Mediation analysis with weighted GLMs in R: What could possibly go wrong?",
    "section": "",
    "text": "This blogpost documents the challenges we faced when attempting to run a mediation analysis using the mediation package with a binomial mediator and outcome while incorporating sampling weights into the analysis with the help of the survey package. Issues and solutions are summarized below. Click the issue number to jump straight to the relevant section of the post."
  },
  {
    "objectID": "posts/2023-12-21_mediation-weighted-glm/index.html#issue-1-correctly-applying-the-fragile-families-weights",
    "href": "posts/2023-12-21_mediation-weighted-glm/index.html#issue-1-correctly-applying-the-fragile-families-weights",
    "title": "Mediation analysis with weighted GLMs in R: What could possibly go wrong?",
    "section": "Issue 1: Correctly applying the Fragile Families weights",
    "text": "Issue 1: Correctly applying the Fragile Families weights\nWe fit the models using survey::svyglm() in order to be able to take the weights into account. First, we need to tell survey that our data is weighted, which we do with the help of the srvyr package for tidyverse-like convenience. Our first attempt looked like this:\ndat_weights <- dat |> \n  srvyr::as_survey_rep(\n    repweights = dplyr::contains(\"_rep\"),\n    weights = m1natwt,\n    combined_weights = TRUE\n  )\nHowever, coming across this CrossValidated question and confirming with the Fragile Families guide to using the weights showed us that the code above fails to tell survey that we want to use jackknife variance estimation. From the guide, p. 2:\n\nAs described in the weights construction memo, the replicate weights require using jackknife estimation of standard errors.\n\nWoops.\nSo, to properly apply the weights, we run this code block instead:\n\n\nCode\ndat_weights <- dat |> \n  srvyr::as_survey_rep(\n    repweights = dplyr::contains(\"_rep\"),\n    weights = m1natwt,\n    combined_weights = TRUE,\n    type = \"JKn\",\n    scales = 1,\n    rscales = 1,\n    mse = TRUE\n)"
  },
  {
    "objectID": "posts/2023-12-21_mediation-weighted-glm/index.html#issue-2-mediation-with-multiple-trial-binomial-mediator-runs-for-weighted-but-not-for-unweighted-models",
    "href": "posts/2023-12-21_mediation-weighted-glm/index.html#issue-2-mediation-with-multiple-trial-binomial-mediator-runs-for-weighted-but-not-for-unweighted-models",
    "title": "Mediation analysis with weighted GLMs in R: What could possibly go wrong?",
    "section": "Issue 2: Mediation with “multiple-trial binomial mediator” runs for weighted but not for unweighted models?",
    "text": "Issue 2: Mediation with “multiple-trial binomial mediator” runs for weighted but not for unweighted models?\nWe knew that we would eventually perform a sensitivity analysis where all models would be run without using the sampling weights. Running the previous steps with the glm() command resulted in an issue:\n\n\nCode\nmodel_m_2 <- glm(\n  formula = informal_support_prop ~ race,\n  weights = informal_support_max,\n  data = dat,\n  family = \"binomial\"\n)\n\nmodel_y_2 <- glm(\n  formula = IPV_prop ~ race + informal_support_prop,\n  weights = IPV_max,\n  data = dat,\n  family = \"binomial\"\n)\n\nmed_2 <- mediation::mediate(\n  model.m = model_m_2, # binomial glm, with weights argument\n  model.y = model_y_2, # binomial glm, with weights argument\n  sims = n_sims,\n  treat = \"race\",\n  mediator = \"informal_support_prop\"\n)\n\n\nError in mediation::mediate(model.m = model_m_2, model.y = model_y_2, : weights on outcome and mediator models not identical\n\n\nIt seems like mediation::mediate() is taking the weights we pass to the GLM function (which correspond to total number of trials, not to sampling weights) as sampling weights, and complaining when it notices that these weights are not the same for the mediator and outcome models. Vasco perhaps clumsily posted this as an issue on the mediation package Github, but at least for now there has been no answer. We can work around this by using an alternative way of specifying the same GLM model — instead of using a proportion as an outcome and passing the number of trials through the weights argument, we can instead use a vector of “successes” and “failures” (which corresponds to the total number of trials minus the number of successes) as the outcome:\n\n\nCode\nmodel_m_2.1 <- glm(\n  formula = cbind(informal_support, informal_support_max - informal_support) ~ race,\n  data = dat,\n  family = \"binomial\"\n)\n\nmodel_y_2.1 <- glm(\n  formula = cbind(IPV, IPV_max - IPV) ~ race + informal_support_prop,\n  data = dat,\n  family = \"binomial\"\n)\n\nmed_2.1 <- mediation::mediate(\n  model.m = model_m_2.1, # binomial glm, with cbind() approach\n  model.y = model_y_2.1, # binomial glm, with cbind() approach\n  sims = n_sims,\n  treat = \"race\",\n  mediator = \"informal_support_prop\"\n)\n\nsummary(med_2.1)\n\n\n\nCausal Mediation Analysis \n\nQuasi-Bayesian Confidence Intervals\n\n                         Estimate 95% CI Lower 95% CI Upper p-value    \nACME (control)            0.00296      0.00149         0.00  <2e-16 ***\nACME (treated)            0.00327      0.00167         0.01  <2e-16 ***\nADE (control)             0.01305      0.00753         0.02  <2e-16 ***\nADE (treated)             0.01335      0.00771         0.02  <2e-16 ***\nTotal Effect              0.01632      0.01052         0.02  <2e-16 ***\nProp. Mediated (control)  0.17991      0.09090         0.33  <2e-16 ***\nProp. Mediated (treated)  0.19794      0.10258         0.35  <2e-16 ***\nACME (average)            0.00311      0.00158         0.00  <2e-16 ***\nADE (average)             0.01320      0.00763         0.02  <2e-16 ***\nProp. Mediated (average)  0.18892      0.09668         0.34  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 1955 \n\n\nSimulations: 1000 \n\n\nSuccess! But at the cost of suspicion. The documentation for the mediate()function warns us that (emphasis added):\n\nAs of version 4.0, the mediator model can be of either ‘lm’, ‘glm’ (or `bayesglm’), ‘polr’ (or `bayespolr’), ‘gam’, ‘rq’, `survreg’, or `merMod’ class, corresponding respectively to the linear regression models, generalized linear models, ordered response models, generalized additive models, quantile regression models, parametric duration models, or multilevel models.. For binary response models, the ‘mediator’ must be a numeric variable with values 0 or 1 as opposed to a factor. Quasi-likelihood-based inferences are not allowed for the mediator model because the functional form must be exactly specified for the estimation algorithm to work. The ‘binomial’ family can only be used for binary response mediators and cannot be used for multiple-trial responses. This is due to conflicts between how the latter type of models are implemented in glm and how ‘mediate’ is currently written.\n\nThus, until the package author clarifies whether this implementation results in correct estimates, we are cautious about trusting them.\n\nSanity check: comparing results with linear probability models\nAs a sanity check, to become a little more confident that mediate is working as intended even though it’s not supposed to work well with a multiple-trial binomial mediator, we compare the results of our model that did run (med_2.1) with a mediation run on linear probability models (basically, using lm() instead of glm() even though the outcomes are proportions.\n\n\nCode\nmodel_m_2.2 <- lm(\n  formula = informal_support_prop ~ race,\n  data = dat\n)\n\nmodel_y_2.2 <- lm(\n  formula = IPV_prop ~ race + informal_support_prop,\n  data = dat\n)\n\nmed_2.2 <- mediation::mediate(\n  model.m = model_m_2.2, # lm\n  model.y = model_y_2.2, # lm\n  sims = n_sims,\n  treat = \"race\",\n  mediator = \"informal_support_prop\"\n)\n\nsummary(med_2.2)\n\n\n\nCausal Mediation Analysis \n\nQuasi-Bayesian Confidence Intervals\n\n               Estimate 95% CI Lower 95% CI Upper p-value    \nACME            0.00319      0.00152         0.01  <2e-16 ***\nADE             0.01377      0.00201         0.03   0.024 *  \nTotal Effect    0.01696      0.00483         0.03   0.006 ** \nProp. Mediated  0.18746      0.07838         0.61   0.006 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 1955 \n\n\nSimulations: 1000 \n\n\nThe results seem fairly similar, but this provides limited comfort.\n\n\nBonus finding: you can specify a mediator that is not technically the variable that your mediator model predicts\nNote that in the models used for med_2.1 there is a potential discrepancy. The mediator model is formulated as formula = cbind(informal_support, informal_support_max - informal_support) ~ race, while the outcome model is formulated as formula = cbind(IPV, IPV_max - IPV) ~ race + informal_support_prop. So the outcome model is using informal_support_prop (our intended mediator) as a predictor of IPV, but the mediator model uses the variables informal_support and informal_support_max to internally generate the proportion of successes. In the call to mediation::mediate() we can then specify mediator = \"informal_support_prop\", and all seems to work as intended. What if we used informal_support as predictor in the outcome model instead? We can then specify mediator = \"informal_support\" and end up with something like this:\n\n\nCode\nmodel_m_2.3 <- glm(\n  formula = cbind(informal_support, informal_support_max - informal_support) ~ race,\n  data = dat,\n  family = \"binomial\"\n)\n\nmodel_y_2.3 <- glm(\n  formula = cbind(IPV, IPV_max - IPV) ~ race + informal_support,\n  data = dat,\n  family = \"binomial\"\n)\n\nmed_2.3 <- mediation::mediate(\n  model.m = model_m_2.3, # binomial glm, with cbind() approach\n  model.y = model_y_2.3, # binomial glm, with cbind() approach\n  sims = n_sims,\n  treat = \"race\",\n  mediator = \"informal_support\"\n)\n\nsummary(med_2.3)\n\n\n\nCausal Mediation Analysis \n\nQuasi-Bayesian Confidence Intervals\n\n                         Estimate 95% CI Lower 95% CI Upper p-value    \nACME (control)           0.001146     0.000607         0.00  <2e-16 ***\nACME (treated)           0.001254     0.000664         0.00  <2e-16 ***\nADE (control)            0.017002     0.009882         0.02  <2e-16 ***\nADE (treated)            0.017110     0.009944         0.02  <2e-16 ***\nTotal Effect             0.018256     0.011170         0.03  <2e-16 ***\nProp. Mediated (control) 0.062331     0.029926         0.12  <2e-16 ***\nProp. Mediated (treated) 0.068606     0.033199         0.12  <2e-16 ***\nACME (average)           0.001200     0.000640         0.00  <2e-16 ***\nADE (average)            0.017056     0.009913         0.02  <2e-16 ***\nProp. Mediated (average) 0.065469     0.031535         0.12  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 1955 \n\n\nSimulations: 1000 \n\n\nLooking at the last three rows (Average Causal Mediated Effect (average), Average Direct Effect (average), and Proportion Mediated (average)), the results seem different than the ones estimated in med_2.1 (which used informal_support_prop as mediator) and med.2.2 (which used linear proability models). But looking closer, you’ll notice that the estimates related to the mediator, namely of the Average Causal Mediated Effect and the Proportion Mediated just seem to be divided by three!\nMy assumption about how that came to be: When we use informal_support_prop, which can only take values on the \\((0, 1)\\) interval, the ACME estimates correspond to the change in probability of IPV associated with going from 0 to 1 on the mediator and thus span the full range of the mediator. When we use informal_support, which takes values in the set \\(\\{0, 1, 2, 3\\}\\), the estimates also correspond to the increase in probability of IPV associated with an increase of 1 on the mediator (e.g., from 0 to 1), but this is only \\(\\frac{1}{3}\\) of the total possible way. Thus, applying this effect three times would be equivalent to moving from 0 to 3 on the mediator scale, the same as moving from 0 to 1 when the mediator is a proportion.4"
  },
  {
    "objectID": "posts/2023-12-21_mediation-weighted-glm/index.html#issue-3-using-a-binary-mediator-and-a-binomial-outcome-works-with-cbindsuccesses-failures-but-not-with-proportion-as-outcome-and-weights-argument-specified",
    "href": "posts/2023-12-21_mediation-weighted-glm/index.html#issue-3-using-a-binary-mediator-and-a-binomial-outcome-works-with-cbindsuccesses-failures-but-not-with-proportion-as-outcome-and-weights-argument-specified",
    "title": "Mediation analysis with weighted GLMs in R: What could possibly go wrong?",
    "section": "Issue 3: Using a binary mediator and a binomial outcome — works with cbind(successes, failures) but not with proportion as outcome and weights argument specified",
    "text": "Issue 3: Using a binary mediator and a binomial outcome — works with cbind(successes, failures) but not with proportion as outcome and weights argument specified\nNext, we attempted to run everything with the same binomial outcome and a binary mediator.\n\n\nCode\nmodel_m_3 <- glm(\n  formula = informal_support_binary ~ race,\n  data = dat,\n  family = \"binomial\"\n)\n\nmodel_y_3 <- glm(\n  formula = IPV_prop ~ race + informal_support_binary,\n  weights = IPV_max,\n  data = dat,\n  family = \"binomial\"\n)\n\nmed_3 <- mediation::mediate(\n  model.m = model_m_3, # binary glm\n  model.y = model_y_3, # binomial glm, with weights argument\n  sims = n_sims,\n  treat = \"race\",\n  mediator = \"informal_support_binary\"\n)\n\n\nError in mediation::mediate(model.m = model_m_3, model.y = model_y_3, : weights on outcome and mediator models not identical\n\n\nOnce again we find that mediate() complains about different weights on mediator and outcome models, even though, according to the documentation, it works with a binary mediator and a binomial outcome. We then try using the vector of successes and failures specification for the outcome model:\n\n\nCode\nmodel_y_3.1 <- glm(\n  formula = cbind(IPV, IPV_max - IPV) ~ race + informal_support_binary,\n  data = dat,\n  family = \"binomial\"\n)\n\nmed_3.1 <- mediation::mediate(\n  model.m = model_m_3,   # binary glm \n  model.y = model_y_3.1, # binomial glm, with cbind() approach\n  sims = n_sims,\n  treat = \"race\",\n  mediator = \"informal_support_binary\"\n)\n\nsummary(med_3.1)\n\n\n\nCausal Mediation Analysis \n\nQuasi-Bayesian Confidence Intervals\n\n                         Estimate 95% CI Lower 95% CI Upper p-value    \nACME (control)            0.00346      0.00152         0.01  <2e-16 ***\nACME (treated)            0.00380      0.00168         0.01  <2e-16 ***\nADE (control)             0.01212      0.00664         0.02  <2e-16 ***\nADE (treated)             0.01247      0.00681         0.02  <2e-16 ***\nTotal Effect              0.01593      0.00988         0.02  <2e-16 ***\nProp. Mediated (control)  0.21804      0.09927         0.37  <2e-16 ***\nProp. Mediated (treated)  0.24095      0.11032         0.39  <2e-16 ***\nACME (average)            0.00363      0.00161         0.01  <2e-16 ***\nADE (average)             0.01230      0.00673         0.02  <2e-16 ***\nProp. Mediated (average)  0.22950      0.10509         0.38  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 1955 \n\n\nSimulations: 1000 \n\n\nThis approach runs, but we are still left wondering whether we can trust the estimates."
  },
  {
    "objectID": "posts/2023-12-21_mediation-weighted-glm/index.html#issue-4-ordinal-mediator-and-binomial-outcome-dont-work-together",
    "href": "posts/2023-12-21_mediation-weighted-glm/index.html#issue-4-ordinal-mediator-and-binomial-outcome-dont-work-together",
    "title": "Mediation analysis with weighted GLMs in R: What could possibly go wrong?",
    "section": "Issue 4: Ordinal mediator and binomial outcome don’t work together",
    "text": "Issue 4: Ordinal mediator and binomial outcome don’t work together\nBecause we would prefer not to binarize the social support variable in order to keep all the information that we can, we also explored using an ordinal mediator model. mediate() works with ordinal models fit with MASS::polr(), so we attempted that.\n\n\nCode\nmodel_m_4 <- MASS::polr(\n  formula = informal_support_ordinal ~ race,\n  data = dat,\n  Hess = TRUE\n)\n\nmodel_y_4 <- glm(\n  formula = cbind(IPV, IPV_max - IPV) ~ race + informal_support_ordinal,\n  data = dat,\n  family = \"binomial\"\n)\n\nmed_4 <- mediation::mediate(\n  model.m = model_m_4, # ordinal polr\n  model.y = model_y_4, # binomial glm, with cbind() approach\n  sims = n_sims,\n  treat = \"race\",\n  mediator = \"informal_support_ordinal\"\n)\n\n\nError in UseMethod(\"isSymmetric\"): no applicable method for 'isSymmetric' applied to an object of class \"c('double', 'numeric')\"\n\n\nThe first issue arises, but it’s easy to fix. Again, from the documentation of the mediate function:\n\nThe quasi-Bayesian approximation (King et al. 2000) cannot be used if ‘model.m’ is of class ‘rq’ or ‘gam’, or if ‘model.y’ is of class ‘gam’, ‘polr’ or ‘bayespolr’. In these cases, either an error message is returned or use of the nonparametric bootstrap is forced. Users should note that use of the nonparametric bootstrap often requires significant computing time, especially when ‘sims’ is set to a large value.\n\nWe can get around this by setting the argument boot to TRUE.\n\n\nCode\nmed_4 <- mediation::mediate(\n  model.m = model_m_4, # ordinal polr\n  model.y = model_y_4, # binomial glm, with cbind() approach\n  sims = n_sims,\n  treat = \"race\",\n  mediator = \"informal_support_ordinal\",\n  boot = TRUE\n)\n\n\nError in cbind(IPV, IPV_max - IPV): object 'IPV' not found\n\n\nNow we get a different problem. It seems the boot package is having trouble with the way we set up our outcome model — hopefully using the other implementation will take care of that.\n\n\nCode\nmodel_y_4.1 <- glm(\n  formula = IPV_prop ~ race + informal_support_ordinal,\n  weights = IPV_max,\n  data = dat,\n  family = \"binomial\"\n)\n\nmed_4 <- mediation::mediate(\n  model.m = model_m_4,   # ordinal polr\n  model.y = model_y_4.1, # binomial glm, with weights argument\n  sims = n_sims,\n  treat = \"race\",\n  mediator = \"informal_support_ordinal\",\n  boot = TRUE\n)\n\n\nError in mediation::mediate(model.m = model_m_4, model.y = model_y_4.1, : weights on outcome and mediator models not identical\n\n\nFinally, we reach a dead-end. It seems once again the mediate() function is taking the prior weights we fed into our glm() as sampling weights and complaining that our ordinal model doesn’t have the same sampling weights. If we are willing to throw away some information, we can make it all work with a binary outcome, thusly:\n\n\nCode\nmodel_y_4.2 <- glm(\n  formula = IPV_binary ~ race + informal_support_ordinal,\n  data = dat,\n  family = \"binomial\"\n)\n\nmed_4 <- mediation::mediate(\n  model.m = model_m_4,   # ordinal polr\n  model.y = model_y_4.2, # binary glm\n  sims = n_sims,\n  treat = \"race\",\n  mediator = \"informal_support_ordinal\",\n  boot = TRUE\n)\n\nsummary(med_4)\n\n\n\nCausal Mediation Analysis \n\nNonparametric Bootstrap Confidence Intervals with the Percentile Method\n\n                          Estimate 95% CI Lower 95% CI Upper p-value\nACME (control)            0.006206    -0.000770         0.01    0.12\nACME (treated)            0.006033    -0.000777         0.01    0.13\nADE (control)             0.012649    -0.026051         0.05    0.54\nADE (treated)             0.012476    -0.025926         0.05    0.54\nTotal Effect              0.018683    -0.022527         0.06    0.41\nProp. Mediated (control)  0.332206    -2.198065         1.90    0.49\nProp. Mediated (treated)  0.322930    -2.248166         1.91    0.49\nACME (average)            0.006120    -0.000772         0.01    0.12\nADE (average)             0.012563    -0.025974         0.05    0.54\nProp. Mediated (average)  0.327568    -2.223116         1.91    0.49\n\nSample Size Used: 1955 \n\n\nSimulations: 1000 \n\n\nThis runs, although the bootstrap adds significantly to computing time and we had to binarize IPV to achieve it."
  },
  {
    "objectID": "posts/2023-12-21_mediation-weighted-glm/index.html#issue-5-mediate-is-not-compatible-with-svyolr-models",
    "href": "posts/2023-12-21_mediation-weighted-glm/index.html#issue-5-mediate-is-not-compatible-with-svyolr-models",
    "title": "Mediation analysis with weighted GLMs in R: What could possibly go wrong?",
    "section": "Issue 5: mediate() is not compatible with svyolr() models",
    "text": "Issue 5: mediate() is not compatible with svyolr() models\nWith this partial success, we checked to see if we could also run this latest mediation on the weighted data.\n\n\nCode\nmodel_m_5 <- survey::svyolr(\n  formula = informal_support_ordinal ~ race,\n  design = dat_weights\n)\n\n\nError in optim(s0, fmin, gmin, method = \"BFGS\", ...): initial value in 'vmmin' is not finite\n\n\nThis throws an error, which this Stack Overflow post can help us solve. We can help the function by providing plausible start values for the coefficients and the intercepts/thresholds. We have one coefficient and three thresholds (from 0 to 1, from 1 to 2, and from 2 to 3), and we know they should be relatively close to zero as well as that the thresholds should be in ascending order. With the syntax start = c(coefficients, thresholds) we can make it work:\n\n\nCode\nmodel_m_5 <- survey::svyolr(\n  formula = informal_support_ordinal ~ race,\n  design = dat_weights,\n  start = c(0, -1, 0, 1)\n)\n\nsummary(model_m_5)\n\n\nCall:\nsvyolr(formula = informal_support_ordinal ~ race, design = dat_weights, \n    start = c(0, -1, 0, 1))\n\nCoefficients:\n             Value Std. Error  t value\nraceBlack -1.62481  0.3874065 -4.19407\n\nIntercepts:\n    Value    Std. Error t value \n0|1  -4.5463   0.3580   -12.6979\n1|2  -3.7384   0.2741   -13.6376\n2|3  -3.0236   0.2458   -12.3027\n\n\nOk, moving on.\n\n\nCode\nmodel_y_5 <- survey::svyglm(\n  formula = IPV_binary ~ race + informal_support_ordinal,\n  design = dat_weights,\n  family = \"binomial\"\n)\n\nmed_5 <- mediation::mediate(\n  model.m = model_m_5, # ordinal svyolr\n  model.y = model_y_5, # binomial svyglm, with weights argument\n  sims = n_sims,\n  treat = \"race\",\n  mediator = \"informal_support_ordinal\",\n  boot = TRUE\n)\n\n\nError in mediation::mediate(model.m = model_m_5, model.y = model_y_5, : weights on outcome and mediator models not identical\n\n\nNot this again 😭. We knew that the weights used for the svyglm() and svyolr() models were the same, so there must be something else happening in the background. Vasco did some investigating and again turned to Stack Overflow. We can see what weights are stored in the model objects generated by svyglm() and svyolr() using the model.frame() function. In the following two code blocks we also use dplyr::slice_head() to display just the first line of results output by model.frame().\n\n\nCode\nmodel.frame(model_m_5) |> dplyr::slice_head() |> gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      informal_support_ordinal\n      race\n      (weights)\n    \n  \n  \n    3\nBlack\n41.13614\n  \n  \n  \n\n\n\n\nCode\nmodel.frame(model_y_5) |> dplyr::slice_head() |> gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      IPV_binary\n      race\n      informal_support_ordinal\n      (weights)\n    \n  \n  \n    1\nBlack\n3\n0.1142592\n  \n  \n  \n\n\n\n\nThe first weight for our ordinal model appears to be 41.13614, while the binomial model uses 0.1142592. We can extract all the weights for both models and compare them. Let’s look at their ratio.\n\n\nCode\nweight_comparison <- tibble::tibble(\n  binomial_weights = model.frame(model_y_5)$`(weights)`,\n  ordinal_weights = model.frame(model_m_5)$`(weights)`,\n  weight_quotient = binomial_weights / ordinal_weights\n)\n\nweight_comparison |> dplyr::slice_head(n = 5) |> gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      binomial_weights\n      ordinal_weights\n      weight_quotient\n    \n  \n  \n    0.11425923\n41.13614\n0.002777587\n    0.05548425\n19.97570\n0.002777587\n    0.03399776\n12.24003\n0.002777587\n    0.33822387\n121.76894\n0.002777587\n    0.23876779\n85.96230\n0.002777587\n  \n  \n  \n\n\n\n\nWould you look at that! The ratio seems to be constant. We can check the entire weight_quotient column just to be extra sure:\n\n\nCode\nweight_comparison |> dplyr::count(weight_quotient) |> gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      weight_quotient\n      n\n    \n  \n  \n    0.002777587\n218\n    0.002777587\n1733\n    0.002777587\n4\n  \n  \n  \n\n\n\n\nThere seem to be three different values that all look the same — perhaps this will make more sense once we learn more about floating point arithmetic, but for now we’ll say the ratio between the weights is always the same. On Stack Overflow, Thomas Lumley, maintainer of the survey package, explains in a reply to my post:\n\nSome functions in the survey package rescale the weights to have unit mean because the large weights (thousands or tens of thousands) in some national surveys can cause convergence problems.\nSince the results aren’t affected at all, the mediate package could probably work around this fairly easily. However, there’s a rescale=FALSE option to svyglm to not rescale the weights, provided for this sort of purpose.\nIf you then have convergence problems in svyglm you could manually rescale the weights to have unit mean before doing any of the analyses.\n\nGood to know!\nLet’s set rescale = FALSE in the svyglm() model and try our luck:\n\n\nCode\nmodel_y_5.1 <- survey::svyglm(\n  formula = IPV_binary ~ race + informal_support_ordinal,\n  design = dat_weights,\n  family = \"binomial\",\n  rescale = FALSE\n)\n\n\nError in is.data.frame(pwts): object 'pwts' not found\n\n\nThis is perplexing, but someone else on Stack Overflow a couple of years ago encountered a similar problem, where object “.survey.prob.weights” was not found when rescale was set to FALSE. After looking at the source code and the accepted answer by user xilliam to the Stack Overflow post, we try to assign the prior weights contained in the survey design object to an object named pwts in the global environment, and that makes svyglm() happy again.\n\n\nCode\npwts <- dat_weights$pweights\n\nmodel_y_5.1 <- survey::svyglm(\n  formula = IPV_binary ~ race + informal_support_ordinal,\n  design = dat_weights,\n  family = \"binomial\",\n  rescale = FALSE\n)\n\n# remove pwts from the global environment\nrm(list = \"pwts\")\n\n\nHowever, we can’t be sure if this correctly solves the problem or introduces new errors. Vasco posted about this issue on Stack Overflow in the hopes that perhaps Thomas Lumley can clarify this for us.\nNow we try to run the mediation.\n\n\nCode\nmed_5 <- mediation::mediate(\n  model.m = model_m_5,   # ordinal svyolr\n  model.y = model_y_5.1, # binary svyglm\n  sims = n_sims,\n  treat = \"race\",\n  mediator = \"informal_support_ordinal\",\n  boot = TRUE\n)\n\n\nError in MASS::polr(formula, data = df, ..., Hess = TRUE, model = FALSE, : formal argument \"data\" matched by multiple actual arguments\n\n\nSomehow it is not working again. What if the outcome model was ordinal as well, just for fun?\n\n\nCode\nmodel_y_5.2 <- survey::svyolr(\n  formula = IPV_ordinal ~ race + informal_support_ordinal,\n  design = dat_weights,\n  start = c(0, 0, 0, 0, seq(from = -1, to = 1, length.out = 24))\n)\n\nmed_5 <- mediation::mediate(\n  model.m = model_m_5,   # ordinal svyolr\n  model.y = model_y_5.2, # ordinal svyolr\n  sims = n_sims,\n  treat = \"race\",\n  mediator = \"informal_support_ordinal\",\n  boot = TRUE\n)\n\n\nError in MASS::polr(formula, data = df, ..., Hess = TRUE, model = FALSE, : formal argument \"data\" matched by multiple actual arguments\n\n\nOk, we give up here."
  },
  {
    "objectID": "posts/2023-12-21_mediation-weighted-glm/index.html#issue-6-options-and-choices-to-boot-to-robustse-or-not-to-robustse",
    "href": "posts/2023-12-21_mediation-weighted-glm/index.html#issue-6-options-and-choices-to-boot-to-robustse-or-not-to-robustse",
    "title": "Mediation analysis with weighted GLMs in R: What could possibly go wrong?",
    "section": "Issue 6: Options and choices to boot — To robustSE or not to robustSE?",
    "text": "Issue 6: Options and choices to boot — To robustSE or not to robustSE?\nFor now, we’ve ignored an optional setting to mediate(): the robustSE argument defaults to FALSE, and, according to the documentation, “If ‘TRUE’, heteroskedasticity-consistent standard errors will be used in quasi-Bayesian simulations. Ignored if ‘boot’ is ‘TRUE’ or neither ‘model.m’ nor ‘model.y’ has a method for vcovHC in the sandwich package.” So we have three options:\n\nboot = TRUE and, automatically, robustSE = FALSE\nboot = FALSE and robustSE = TRUE, and\nboot = FALSE and robustSE = FALSE.\n\nAnd we know that, when chosen, the heteroskedasticity-consistent standard errors will be calculated with the help of the sandwich package.\nAccording to the mediation package vignettes, the authors recommend setting boot = TRUE: “In general, as long as computing power is not an issue, analysts should estimate confidence intervals via the bootstrap with more than 1000 resamples, which is the default number of simulations.” So we could do that. Unfortunately, computing power is an issue, as we wanted to run a multiverse analysis that meant we would use the mediate() function a bunch of times. To figure out whether to set robustSE to TRUE or FALSE, it could be useful to see which option gives use results that are closer to the bootstrap option. Let’s use a binary mediator and outcome, just so we are sure there aren’t other issues at play.\nFor convenience, we’ll create a function called compare_meds() that takes in different mediation models and outputs a table.\n\n\nCode\nget_meds <- function(med_model){\n  \n  boot <- as.character(med_model[[\"boot\"]])\n  \n  robustSE <- as.character(med_model[[\"robustSE\"]])\n  \n  results_te <- tibble::tibble(\n    effect = \"total_effect\",\n    estimate = med_model[[\"tau.coef\"]],\n    ci.low = med_model[[\"tau.ci\"]][[1]],\n    ci.high = med_model[[\"tau.ci\"]][[2]],\n    p.value = med_model[[\"tau.p\"]]\n  )\n  \n  results_de <- tibble::tibble(\n    effect = \"direct_effect\",\n    estimate = med_model[[\"z.avg\"]],\n    ci.low = med_model[[\"z.avg.ci\"]][[1]],\n    ci.high = med_model[[\"z.avg.ci\"]][[2]],\n    p.value = med_model[[\"z.avg.p\"]]\n  )\n  \n  results_ie <- tibble::tibble(\n    effect = \"indirect_effect\",\n    estimate = med_model[[\"d.avg\"]],\n    ci.low = med_model[[\"d.avg.ci\"]][[1]],\n    ci.high = med_model[[\"d.avg.ci\"]][[2]],\n    p.value = med_model[[\"d.avg.p\"]]\n  )\n  \n  results <- dplyr::bind_rows(results_te, results_de, results_ie)\n  \n  output <- results |> \n    dplyr::mutate(boot = boot, robustSE = robustSE) |> \n    dplyr::relocate(boot, robustSE)\n  \n  return(output)\n}\n\ncompare_meds <- function(med_models, effect){\n  if (class(med_models) != \"list\") {\n    stop(\"Argument should be a list of models\")\n  }\n  \n  med_models_df <- med_models  |> \n    purrr::map(.f = ~ get_meds(.x)) |> \n    purrr::list_rbind() |> \n    dplyr::filter(effect == !!effect)\n  \n  return(med_models_df)\n}\n\n\nFirst, we compare unweighted models. To reduce simulation-dependent variability, we set sims = 3000.\n\n\nCode\nmodel_m_6 <- glm(\n  formula = informal_support_binary ~ race,\n  data = dat,\n  family = \"binomial\"\n)\n\nmodel_y_6 <- glm(\n  formula = IPV_binary ~ race + informal_support_binary,\n  data = dat,\n  family = \"binomial\"\n)\n\nmed_6.1 <- mediation::mediate(\n  model.m = model_m_6, # binary glm\n  model.y = model_y_6, # binary glm\n  sims = 3000,\n  boot = TRUE,\n  treat = \"race\",\n  mediator = \"informal_support_binary\"\n)\n\nmed_6.2 <- mediation::mediate(\n  model.m = model_m_6, # binary glm\n  model.y = model_y_6, # binary glm\n  sims = 3000,\n  robustSE = TRUE,\n  treat = \"race\",\n  mediator = \"informal_support_binary\"\n)\n\nmed_6.3 <- mediation::mediate(\n  model.m = model_m_6, # binary glm\n  model.y = model_y_6, # binary glm\n  sims = 3000,\n  robustSE = FALSE,\n  treat = \"race\",\n  mediator = \"informal_support_binary\"\n)\n\nmod_list <- list(med_6.1, med_6.2, med_6.3)\n\ncompare_meds(mod_list, effect = \"total_effect\") |> gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      boot\n      robustSE\n      effect\n      estimate\n      ci.low\n      ci.high\n      p.value\n    \n  \n  \n    TRUE\nFALSE\ntotal_effect\n0.01652099\n-0.02547328\n0.05425143\n0.5026667\n    FALSE\nTRUE\ntotal_effect\n0.01442257\n-0.02563536\n0.05514849\n0.4973333\n    FALSE\nFALSE\ntotal_effect\n0.01500741\n-0.02669079\n0.05575647\n0.4786667\n  \n  \n  \n\n\n\n\nCode\ncompare_meds(mod_list, effect = \"direct_effect\") |> gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      boot\n      robustSE\n      effect\n      estimate\n      ci.low\n      ci.high\n      p.value\n    \n  \n  \n    TRUE\nFALSE\ndirect_effect\n0.01067619\n-0.02962899\n0.05054697\n0.6346667\n    FALSE\nTRUE\ndirect_effect\n0.01044635\n-0.03001886\n0.05129118\n0.6180000\n    FALSE\nFALSE\ndirect_effect\n0.01100672\n-0.03089254\n0.05183531\n0.5966667\n  \n  \n  \n\n\n\n\nCode\ncompare_meds(mod_list, effect = \"indirect_effect\") |> gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      boot\n      robustSE\n      effect\n      estimate\n      ci.low\n      ci.high\n      p.value\n    \n  \n  \n    TRUE\nFALSE\nindirect_effect\n0.005844795\n5.436462e-05\n0.009166102\n0.04666667\n    FALSE\nTRUE\nindirect_effect\n0.003976221\n-2.062479e-04\n0.009129695\n0.06600000\n    FALSE\nFALSE\nindirect_effect\n0.004000689\n-1.477036e-04\n0.009029200\n0.06866667\n  \n  \n  \n\n\n\n\nWe can see that the results are very similar, and in fact running the mediations several times reveals that the difference seems to be due to simulation variability. Given this, Vasco would personally rather specify robustSE = FALSE, since it does not make sense to correct for heteroskedasticity in a logistic regression model which cannot suffer from heteroskedasticity in the first place.\nWhat about if we use weighted models?\n\n\nCode\nmodel_m_6.1 <- survey::svyglm(\n  formula = informal_support_binary ~ race,\n  design = dat_weights,\n  family = \"binomial\"\n)\n\nmodel_y_6.1 <- survey::svyglm(\n  formula = IPV_binary ~ race + informal_support_binary,\n  design = dat_weights,\n  family = \"binomial\"\n)\n\nmed_6.4 <- mediation::mediate(\n  model.m = model_m_6.1, # binary svyglm\n  model.y = model_y_6.1, # binary svyglm\n  sims = 3000,\n  boot = TRUE,\n  treat = \"race\",\n  mediator = \"informal_support_binary\"\n)\n\nmed_6.5 <- mediation::mediate(\n  model.m = model_m_6.1, # binary svyglm\n  model.y = model_y_6.1, # binary svyglm\n  sims = 3000,\n  robustSE = TRUE,\n  treat = \"race\",\n  mediator = \"informal_support_binary\"\n)\n\nmed_6.6 <- mediation::mediate(\n  model.m = model_m_6.1, # binary svyglm\n  model.y = model_y_6.1, # binary svyglm\n  sims = 3000,\n  robustSE = FALSE,\n  treat = \"race\",\n  mediator = \"informal_support_binary\"\n)\n\nmod_list <- list(med_6.4, med_6.5, med_6.6)\n\ncompare_meds(mod_list, effect = \"total_effect\") |> gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      boot\n      robustSE\n      effect\n      estimate\n      ci.low\n      ci.high\n      p.value\n    \n  \n  \n    TRUE\nFALSE\ntotal_effect\n0.04098304\n0.03768277\n0.04755021\n0.0000000\n    FALSE\nTRUE\ntotal_effect\n0.04262195\n0.02844796\n0.05749040\n0.0000000\n    FALSE\nFALSE\ntotal_effect\n0.04250050\n-0.07473692\n0.15539120\n0.4853333\n  \n  \n  \n\n\n\n\nCode\ncompare_meds(mod_list, effect = \"direct_effect\") |> gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      boot\n      robustSE\n      effect\n      estimate\n      ci.low\n      ci.high\n      p.value\n    \n  \n  \n    TRUE\nFALSE\ndirect_effect\n0.02799638\n0.02774052\n0.02807322\n0.0000000000\n    FALSE\nTRUE\ndirect_effect\n0.02797940\n0.01498423\n0.04188653\n0.0006666667\n    FALSE\nFALSE\ndirect_effect\n0.02830803\n-0.09142372\n0.14196905\n0.6226666667\n  \n  \n  \n\n\n\n\nCode\ncompare_meds(mod_list, effect = \"indirect_effect\") |> gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      boot\n      robustSE\n      effect\n      estimate\n      ci.low\n      ci.high\n      p.value\n    \n  \n  \n    TRUE\nFALSE\nindirect_effect\n0.01298666\n0.009710929\n0.01970304\n0.0000000\n    FALSE\nTRUE\nindirect_effect\n0.01464254\n0.009692816\n0.01985872\n0.0000000\n    FALSE\nFALSE\nindirect_effect\n0.01419247\n-0.005836507\n0.04106818\n0.1626667\n  \n  \n  \n\n\n\n\nWhile the estimates are similar, the confidence intervals are much tighter when boot or robustSE are set to TRUE, and suddenly all estimates are significant! What gives?\nFirst, an answer by Thomas Lumley (maintainer of the survey package) on Stack Overflow suggests that bootstrapping doesn’t work well with these survey-weighted models, although his answer does not fully map to our situation. In another answer, this time about applying robust standard errors with the sandwich package5, Lumley is clearer: “For svyglm models, vcov() already produces the appropriate sandwich estimator, and I don’t think the”sandwich” package knows enough about the internals of the object to get it right.” A few years prior, Achim Zeileis, maintainer of the sandwich package, had given a different perspective which also boils down to do not use sandwich on models fit with survey.\nThere you have it, boot = FALSE and robustSE = FALSE is the way to go.\n\nBonus: this goes for using marginaleffects as well\nOften we are not interested in mediation, but rather would like a clean and interpretable way to report results from our GLMs. One could not be faulted for preferring to do this on the predicted probabilities scale rather than dealing with odds-ratios and other headaches. For this, the marginaleffects package can be of enormous help.\nFor example, we can examine the predicted probability of experiencing IPV for Black and White women:\n\n\nCode\nmarginaleffects::avg_predictions(\n  model = model_y_6.1, # binary svyglm\n  variables = \"race\"\n) |> gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      race\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    White\n0.6986166\n0.03988122\n17.51743\n1.054819e-68\n0.6204508\n0.7767824\n    Black\n0.7264507\n0.04437891\n16.36928\n3.169473e-60\n0.6394697\n0.8134318\n  \n  \n  \n\n\n\n\nAnd then compare the probabilities themselves, for which we again have the option of “correcting” the standard errors via the vcov argument. Look at the difference in the confidence intervals and p-values spit out when we don’t or do specify vcov = \"HC\":\n\n\nCode\nmarginaleffects::avg_comparisons(\n  model = model_y_6.1, # binary svyglm\n  variables = \"race\"\n) |> gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      term\n      contrast\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    race\nBlack - White\n0.02783414\n0.06101841\n0.4561596\n0.6482752\n-0.09175976\n0.147428\n  \n  \n  \n\n\n\n\nCode\nmarginaleffects::avg_comparisons(\n  model = model_y_6.1, # binary svyglm\n  variables = \"race\",\n  vcov = \"HC\"\n) |> gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      term\n      contrast\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    race\nBlack - White\n0.02783414\n0.006864551\n4.054765\n5.018481e-05\n0.01437986\n0.04128841\n  \n  \n  \n\n\n\n\nThe estimates are the same, but the standard error (wrongly) becomes minuscule if we apply the “correction” for heteroskedasticity without realizing that sandwich does not work properly on models fit with survey."
  },
  {
    "objectID": "posts/2023-12-21_mediation-weighted-glm/index.html#issue-7-bounds-of-the-confidence-intervals-are-rounded-inconsistently",
    "href": "posts/2023-12-21_mediation-weighted-glm/index.html#issue-7-bounds-of-the-confidence-intervals-are-rounded-inconsistently",
    "title": "Mediation analysis with weighted GLMs in R: What could possibly go wrong?",
    "section": "Issue 7: Bounds of the confidence intervals are rounded inconsistently",
    "text": "Issue 7: Bounds of the confidence intervals are rounded inconsistently\nDid you notice? Each time we use summary() on a mediate() object, the lower bound of the 95% CI is reported with 5 decimal places, while the upper bound is reported with 2 decimal places. This also bothered Stack Overflow user sateayam over 4 years ago, and luckily user Roland answered with a fix:\n\n\nCode\ntrace(mediation:::print.summary.mediate, \n      at = 11,\n      tracer = quote({\n        printCoefmat <- function(x, digits) {\n          p <- x[, 4]\n          x[, 1:3] <- sprintf(\"%.6f\", x[, 1:3])\n          x[, 4] <- sprintf(\"%.2f\", p)\n          print(x, quote = FALSE, right = TRUE)\n        } \n      }),\n      print = FALSE)\n\n\n[1] \"print.summary.mediate\"\n\n\nCode\nmediation:::print.summary.mediate(summary(med_6.6))\n\n\n\nCausal Mediation Analysis \n\nQuasi-Bayesian Confidence Intervals\n\n                         Estimate 95% CI Lower 95% CI Upper p-value\nACME (control)           0.014568    -0.006243     0.041058    0.16\nACME (treated)           0.013817    -0.005456     0.040683    0.16\nADE (control)            0.028684    -0.092798     0.144320    0.62\nADE (treated)            0.027932    -0.088380     0.139389    0.62\nTotal Effect             0.042501    -0.074737     0.155391    0.49\nProp. Mediated (control) 0.146208    -3.221979     2.785668    0.57\nProp. Mediated (treated) 0.120963    -3.442961     2.920213    0.57\nACME (average)           0.014192    -0.005837     0.041068    0.16\nADE (average)            0.028308    -0.091424     0.141969    0.62\nProp. Mediated (average) 0.133585    -3.310359     2.869012    0.57\n\nSample Size Used: 1955 \n\n\nSimulations: 3000 \n\n\nCode\nuntrace(mediation:::print.summary.mediate)\n\n\nUnfortunately, this solution also changes the way p-values are presented. Look again at the results from med_6.5 and compare with the following:\n\n\nCode\ntrace(mediation:::print.summary.mediate, \n      at = 11,\n      tracer = quote({\n        printCoefmat <- function(x, digits) {\n          p <- x[, 4]\n          x[, 1:3] <- sprintf(\"%.6f\", x[, 1:3])\n          x[, 4] <- sprintf(\"%.2f\", p)\n          print(x, quote = FALSE, right = TRUE)\n        } \n      }),\n      print = FALSE)\n\n\n[1] \"print.summary.mediate\"\n\n\nCode\nmediation:::print.summary.mediate(summary(med_6.5))\n\n\n\nCausal Mediation Analysis \n\nQuasi-Bayesian Confidence Intervals\n\n                         Estimate 95% CI Lower 95% CI Upper p-value\nACME (control)           0.015141     0.010027     0.020603    0.00\nACME (treated)           0.014144     0.009419     0.019075    0.00\nADE (control)            0.028478     0.015166     0.042588    0.00\nADE (treated)            0.027481     0.014640     0.041187    0.00\nTotal Effect             0.042622     0.028448     0.057490    0.00\nProp. Mediated (control) 0.355961     0.239907     0.512000    0.00\nProp. Mediated (treated) 0.331810     0.218765     0.493242    0.00\nACME (average)           0.014643     0.009693     0.019859    0.00\nADE (average)            0.027979     0.014984     0.041887    0.00\nProp. Mediated (average) 0.343885     0.229308     0.502018    0.00\n\nSample Size Used: 1955 \n\n\nSimulations: 3000 \n\n\nCode\nuntrace(mediation:::print.summary.mediate)\n\n\nThe interim solution is to print the summary twice, once with the original function for expected p-value behavior, and another time with the edited function for confidence-interval consistency. Someone more well-versed might want to take a shot at adapting the code so that the behavior is maintained for the p-values6 and fixed for the confidence intervals."
  },
  {
    "objectID": "posts/2023-02-21_adhd-norm/index.html",
    "href": "posts/2023-02-21_adhd-norm/index.html",
    "title": "What if ADHD was the norm?",
    "section": "",
    "text": "This post is an adaptation / archive of something I first wrote for Instagram (link to original post). The images are reposted in order with respective image descriptions.\nTikTok account @myfavouritejo (also on Instagram @myfavouritejo) asks: what if neurodivergence was the norm? Highly inspired by their content, I made this post to subvert how we generally conceive of Attention Deficit / Hyperactivity Disorder. If 95% of people were like that, would we create a diagnosis of “Neurotypical Spectrum Disorder”? What other characteristics come to mind?\n\n\n\nImage text:\nHow to Spot a Neurotypical - 5 Signs of Neurotypical Spectrum Disroder\n(POV: ADHD* is the norm)\n*Attention Deficit/Hyperactivity Disorder\n\n\n\n\n\nImage text:\n1. Lack of concentration\nNeurotypicals lack the normal ability to focus for hours and days on tasks that interest them. Often, they need to break up large goals into smaller steps that can be accomplished over weeks. This different kind of attention, also called “hypofocus”, allows them to pay attention to fewer things, with less intensity, but over a longer period of time (days, weeks, months, even), which is why some people with NSD consider hypofocus a “superpower”.\n\n\n\n\n\nImage text:\n2. Restricted interests\nIt’s common for neurotypicals to have just a few interests over the course of their adult lives. Even the few interests they have, they must enjoy moderately, lacking the normal intensity. Neurotypicals seem to prefer not to engage with their hobbies more than once or twice a week. Although this tendency hinders the normal development of a wide breadth of knowledge over the lifespan, people with NSD can compensate for this deficit by developing mastery in one or two domains.\n\n\n\n\n\nImage text:\n3. Slow learning rate\nNeurotypicals find it difficult to learn about topics that interest them with the speed that normal people are used to. They prefer structured, slow (think weeks, not days), guided learning to learn the same amount that a normal person could over a single weekend. If your neurotypical friend mentioned that they started learning how to crochet, don’t expect to receive a sweater any time soon. Be patient, they’ll get there, eventually.\n\n\n\n\n\nImage text:\n4. Atypical conversation patterns\nLikely due to information processing difficulties, neurotypical people tend to speak slowly, in a monotone, without using speech modulation (changing the speed and volume of speech, using varied accents, making random sounds) to have fun, express their enthusiasm, or express themselves more richly. They tend to only tell one story at a time, very linearly, prioritizing simplicity and depriving their conversation partners of highly interesting tangents.\n\n\n\n\n\nImage text:\n5. Low creativity\nPeople with Neurotypical Spectrum Disorder, despite saying they value creativity, suffer from a highly limiting belief: they believe in a “box” inside of which thinking occurs, and which contains a very limited number of options. They associate great discomfort with what they call “thinking outside of the box”, meaning that their belief in this invisible box, irrational as it is, has real consequences, restricting the ways they think and act in the world.\n\n\n\n\n\nImage text:\nPeople with adhd live in a world designed by and for neurotypical people.\n\nTo navigate the world, they must learn how others act and pretend to be like them.\n\nWhat if neurotypicals learned more about adhd? what if we created a world that accommodates neurodiversity?\n\n\nNote: Colors are from this color palette, from Almodóvar’s Pain and Glory: https://www.kwanyuanliu.com/post/pain-glory-colors\n\n\n\nReusehttps://creativecommons.org/licenses/by-nc/4.0/CitationBibTeX citation:@online{brazão2023,\n  author = {Vasco Brazão},\n  title = {What If {ADHD} Was the Norm?},\n  date = {2023-02-21},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVasco Brazão. 2023. “What If ADHD Was the Norm?” February\n21, 2023."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog by Vasco Brazão",
    "section": "",
    "text": "Mediation analysis with weighted GLMs in R: What could possibly go wrong?\n\n\nIssues (and some solutions) when working with R packages {mediation} and {survey}\n\n\n\n\nstatistics\n\n\nmediation\n\n\nsurvey\n\n\nglm\n\n\nfragile families\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2023\n\n\nVasco Brazão, Priya Devendran\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedium Friend Links\n\n\nOr: Read my Medium posts for free\n\n\n\n\n\n\n\n\n\nSep 26, 2023\n\n\nVasco Brazão\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDevaneios pós twitter e pré aula de ucraniano\n\n\n\n\n\n\n\nportuguese\n\n\nwar\n\n\npoetry\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\nVasco Brazão\n\n\n\n\n\n\n  \n\n\n\n\nAnti-fat bias, not ‘obesity’, is a problem worth fighting\n\n\nHere’s what changed my mind\n\n\n\n\ni used to think\n\n\nenglish\n\n\nhealth\n\n\nfat liberation\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nVasco Brazão\n\n\n\n\n\n\n  \n\n\n\n\nWhat if Autism was the norm?\n\n\nHow might we describe “Neurotypical Spectrum Disorder” in a world where most people are Autistic?\n\n\n\n\nautism\n\n\nneurodiversity\n\n\nenglish\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\nVasco Brazão\n\n\n\n\n\n\n  \n\n\n\n\nWhat if ADHD was the norm?\n\n\nHow might we describe “Neurotypical Spectrum Disorder” in a world where most people have ADHD?\n\n\n\n\nadhd\n\n\nneurodiversity\n\n\nenglish\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\nVasco Brazão\n\n\n\n\n\n\n  \n\n\n\n\nI used to think tidying was moral\n\n\nIt’s not. It’s functional. Let KC Davis tell you what that means.\n\n\n\n\ni used to think\n\n\nenglish\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\nVasco Brazão\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "One day this will say something about me and/or this blog!\nFor now, feel free to use the clickity things below to click away!"
  }
]