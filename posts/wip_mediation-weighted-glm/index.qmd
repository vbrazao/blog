---
title: "Mediation analysis with weighted GLMs in R: What could possibly go wrong?"
subtitle: "Issues (and some solutions) when working with R packages {mediation} and {survey}"
author: 
 - name: Vasco Braz√£o
 - name: Priya Devendran
date: "2023-12-21"
categories: [statistics, mediation, survey, glm, fragile families]
draft: true
execute: 
  warning: false
format:
  html:
    code-fold: show
---

This blogpost documents the challenges we faced when attempting to run a mediation analysis using the `mediation` package with a binomial mediator and outcome while incorporating sampling weights into the analysis with the help of the `survey` package. Issues and solutions are summarized below. Click the issue number to jump straight to the relevant section of the post.

::: callout-note
## Issues and solutions: summary

[**Issue 1: Correctly applying the Fragile Families weights**]

**Description**: We need to specify jackknife estimation of standard errors for `survey` to properly handle our replicate weights.

**Solution**: Besides specifying the `weights` and `repweights` arguments, we also have to specify `combined_weights = TRUE`, `type = "JKn"`, `scales = 1`, `rscales = 1`, and `mse = TRUE`.

[**Issue 2: Mediation with "multiple-trial binomial mediator" runs for weighted but not for unweighted models?**]

**Description**: When using multiple-trial binomial GLMs for the mediator and outcome models, `mediate()` complains: "`weights on outcome and mediator models not identical`".

**Solution**: The problem occurs when we specify the GLMs with a proportion as the outcome and the total number of trials given in the `weights` argument, if the number of trials is different for both models. We can overcome this by specifying the outcome as `cbind(successes, failures)`. However, according to the package documentation, `mediate()` should not accept multiple-trial binomial mediators in the first place, leaving us with some doubts about whether all the estimates are calculated correctly in this case.

[**Issue 3: Using a binary mediator and a binomial outcome --- works with cbind(successes, failures) but not with proportion as outcome and weights argument specified**](#issue-3-using-a-binary-mediator-and-a-binomial-outcome-works-with-cbindsuccesses-failures-but-not-with-proportion-as-outcome-and-weights-argument-specified)

**Description**: When using a single trial binomial GLM for the mediator model and a multiple-trial binomial GLM for the outcome model, `mediate()` complains: "`weights on outcome and mediator models not identical`".

**Solution**: As in Issue 2, the issue is solved by using the `cbind(successes, failures)` specification for the outcome model.

[**Issue 4: Ordinal mediator and binomial outcome don't work together**](#issue-4-ordinal-mediator-and-binomial-outcome-dont-work-together)

**Description**: When the mediator model is ordinal (fit with `MASS::polr()`, `mediate()` will not work with a binomial outcome model either way it's specified.

**Solution**: No solution was found, other than using an ordinal mediator model with a binary outcome model.

[**Issue 5: mediate() is not compatible with svyolr() models**](#issue-5-mediate-is-not-compatible-with-svyolr-models)

**Description**: It seems that `mediate()` simply won't work with models fit with `survey::svyolr()`.

**Solution**: No solution was found.

[**Issue 6: Options and choices to boot --- To robustSE or not to robustSE?**](#issue-6-options-and-choices-to-boot-to-robustse-or-not-to-robustse)

**Description**: `mediate()` can work via boostrap or quasi-Bayesian approximation, and has an option to calculate "robust standard errors" for the latter. Which options work with survey-weighted GLMs?

**Solution**: Bootstrapping should probably not be used. `robustSE` should be set to `FALSE`. If `marginaleffects` is used for comparisons, `vcov` should be set to `vcov(model)` or `NULL`, as the `sandwich` package does not correctly compute robust standard errors for models fit with `survey`.

[**Issue 7: Boundaries of the confidence intervals are rounded differently**]

**Description**: When you apply `summary()` to an object produced by `mediate()`, the upper bound of the confidence intervals is rounded to only two decimal places, whereas there are more decimal places for the lower bound.

**Solution**: We can modify the function `print.summary.mediate()` to get around this problem, although the current solution then messes with how the p-values are reported.
:::

# Background

```{r setup}
#| message: false

# load the packages we will use
library(tidyverse)
library(here)
library(mediation)
library(survey)
library(srvyr)
library(gt)
library(MASS)
library(marginaleffects)


# directory for the post (to change once post goes live)
post_folder <- "wip_mediation-weighted-glm"

# number of simulations for mediation analysis
n_sims <- 1000
```

As part of a research program seeking to better understand intimate partner violence (IPV) through an intersectional lens, we wanted to use the [Future of Families & Child Wellbeing Study](https://ffcws.princeton.edu/)[^1] dataset to run a mediation analysis. The data come from a survey with a complex design, and baseline and replicate weights are provided for each participant in the "national" sample.[^2] These weights should allow us to make estimates nationally representative. Our mediator and outcome variables could reasonably be construed as binomial outcomes, and this was our preferred approach to fitting the necessary models. However, we encountered several difficulties along the way, and attempt to document and make sense of them here.

[^1]: Previously called the "Fragile Families and Child Wellbeing Study".

[^2]: For more information about the weights, have a look at the [methodology document linked here](https://ffcws.princeton.edu/sites/g/files/toruqf4356/files/ff_const_wgts.pdf).

For demonstrative purposes we created a synthetic dataset using the `synthpop` package. This dataset mirrors the characteristics of our data without it being possible to identify any real individuals from it.

```{r data}
# get synthetic dataset
dat <- readRDS(here::here("posts", post_folder, "data_reduced_synth.RDS")) |> 
  # create other variables we will need
  dplyr::mutate(
    informal_support_prop = informal_support/3,
    informal_support_max = 3,
    informal_support_binary = ifelse(informal_support == 3, 1, 0),
    informal_support_ordinal = forcats::as_factor(informal_support) |> 
      forcats::fct_relevel("0", "1", "2", "3"),
    IPV_prop = IPV/24,
    IPV_max = 25,
    IPV_binary = ifelse(IPV > 0, 1, 0),
    IPV_ordinal = forcats::as_factor(IPV) |> 
      forcats::fct_expand("0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24") |> 
      forcats::fct_relevel("0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24")
  ) |> 
  # move all the non-weight variables to the beginning
  dplyr::relocate(
    !dplyr::starts_with("m1"),
    .before = "m1natwt"
  )
```

We have three variables that will be used for mediation, `race` ("White" or "Black"), `informal_support` (an integer from 0 to 3), and `IPV` (an integer from 0 to 24). We manually create `_prop` and `_max` variables for informal support and IPV, so that we can use the proportion as an outcome in our GLMs and use the `weights` argument to specify the number of trials. We also create `informal_support_binary`, which classifies women as having high access to informal support (takes the value 1) when they score 3 out of 3, and low access (takes the value 0) when they score less than 3, as well as `IPV_binary`, which classifies women as having experienced some form of IPV (takes the value 1) if they score more than 0 and as not having experienced IPV (takes the value 0) if they score exactly 0. These binarized variables will become useful later on. Finally, we create the ordered factors `informal_support_ordinal` and `IPV_ordinal` to be used in ordinal models later on.

Each person also has a baseline weight, `m1natwt`, and 33 replicate weights, `m1natwt_rep1` and so on. Behold:

```{r data.head}
head(dat) |> 
  gt::gt()
```

We'll need to run two models --- the first predicting the mediator, the second predicting the outcome --- and feed these models into `mediation::mediate()`.

## Issue 1: Correctly applying the Fragile Families weights

We fit the models using `survey::svyglm()` in order to be able to take the weights into account. First, we need to tell `survey` that our data is weighted, which we do with the help of the `srvyr` package for tidyverse-like convenience. Our first attempt looked like this:

``` r
dat_weights <- dat |> 
  srvyr::as_survey_rep(
    repweights = dplyr::contains("_rep"),
    weights = m1natwt,
    combined_weights = TRUE
  )
```

However, coming across this [CrossValidated question](https://stats.stackexchange.com/questions/409463/duplicating-stata-survey-design-using-svrepdesign-from-survey-package-in-r) and confirming with the Fragile Families guide to using the weights showed us that the code above fails to tell `survey` that we want to use jackknife variance estimation. From the [guide](https://fragilefamilies.princeton.edu/sites/fragilefamilies/files/using_the_fragile_families_weights_waves_1_6.pdf), p. 2:

> As described in the weights construction memo, the replicate weights require using jackknife estimation of standard errors.

Woops.

So, to properly apply the weights, we run this code block instead:

```{r data.weights}
dat_weights <- dat |> 
  srvyr::as_survey_rep(
    repweights = dplyr::contains("_rep"),
    weights = m1natwt,
    combined_weights = TRUE,
    type = "JKn",
    scales = 1,
    rscales = 1,
    mse = TRUE
)
```

# Running the weighted models and mediation

With the weighted dataframe in hand, we can use `survey::svyglm()` to fit our GLMs predicting `informal_support` and `IPV`.

Our informal support variable is the sum of three items for which each mother could get a 1 or a 0 (indicating that she did or did not have access to that form of support). As such, it seems sensible to model it as a binomial outcome --- each mother has a certain number of "successes" out of three possible trials.

```{r model.m.ideal}
model_m_1 <- survey::svyglm(
  formula = informal_support_prop ~ race,
  weights = informal_support_max,
  design = dat_weights,
  family = "binomial"
)

summary(model_m_1)
```

We get the following warning many times:

```         
Warning: non-integer #successes in a binomial glm!
```

This is `survey`'s way of asking us to specify the family as "quasibinomial" instead of "binomial", since our outcome is a proportion (weighted by the number of trials) and not binary. However, if we do that, `mediation` will not work with our models.

Our IPV variable is the sum of twelve items for which each mother could get a score of 0, 1, or 2 (indicating how often she experienced a given form of violence from her partner). As such, it also seems sensible to model it as a binomial outcome --- each mother has a certain number of "successes" out of 24 possible "trials".[^3]

[^3]: Another option, which we won't go into, would be to take the responses to the items themselves and use a multi-level ordinal model with item as the random grouping factor.

```{r model.y.ideal}
model_y_1 <- survey::svyglm(
  formula = IPV_prop ~ race + informal_support_prop,
  weights = IPV_max,
  design = dat_weights,
  family = "binomial"
)

summary(model_y_1)
```

(Here again we suppressed the warning that would have otherwise popped up.)

Finally, our mediation:

```{r mediation.ideal}
#| warning: true

med_1 <- mediation::mediate(
  model.m = model_m_1, # binomial svyglm, with weights argument
  model.y = model_y_1, # binomial svyglm, with weights argument
  sims = n_sims,
  treat = "race",
  mediator = "informal_support_prop"
)

summary(med_1)
```

A small but statistically significant mediation effect! The warning about weights not being taken as total number of trials makes us nervous, but then again, we did give the models sampling weights, so maybe it's ok?

## Issue 2: Mediation with "multiple-trial binomial mediator" runs for weighted but not for unweighted models?

We knew that we would eventually perform a sensitivity analysis where all models would be run without using the sampling weights. Running the previous steps with the `glm()` command resulted in an issue:

```{r unweighted.mediation.ideal.fail}
#| error: true

model_m_2 <- glm(
  formula = informal_support_prop ~ race,
  weights = informal_support_max,
  data = dat,
  family = "binomial"
)

model_y_2 <- glm(
  formula = IPV_prop ~ race + informal_support_prop,
  weights = IPV_max,
  data = dat,
  family = "binomial"
)

med_2 <- mediation::mediate(
  model.m = model_m_2, # binomial glm, with weights argument
  model.y = model_y_2, # binomial glm, with weights argument
  sims = n_sims,
  treat = "race",
  mediator = "informal_support_prop"
)
```

It seems like `mediation::mediate()` is taking the weights we pass to the GLM function (which correspond to total number of trials, not to sampling weights) as sampling weights, and complaining when it notices that these weights are not the same for the mediator and outcome models. Vasco perhaps clumsily posted this as an [issue on the mediation package Github](https://github.com/kosukeimai/mediation/issues/59), but at least for now there has been no answer. We can work around this by using an alternative way of specifying the same GLM model --- instead of using a proportion as an outcome and passing the number of trials through the `weights` argument, we can instead use a vector of "successes" and "failures" (which corresponds to the total number of trials minus the number of successes) as the outcome:

```{r unweighted.mediation.ideal.workaround}
model_m_2.1 <- glm(
  formula = cbind(informal_support, informal_support_max - informal_support) ~ race,
  data = dat,
  family = "binomial"
)

model_y_2.1 <- glm(
  formula = cbind(IPV, IPV_max - IPV) ~ race + informal_support_prop,
  data = dat,
  family = "binomial"
)

med_2.1 <- mediation::mediate(
  model.m = model_m_2.1, # binomial glm, with cbind() approach
  model.y = model_y_2.1, # binomial glm, with cbind() approach
  sims = n_sims,
  treat = "race",
  mediator = "informal_support_prop"
)

summary(med_2.1)
```

Success! But at the cost of suspicion. The [documentation for the `mediate`](https://www.rdocumentation.org/packages/mediation/versions/4.5.0/topics/mediate)`()`[ function](https://www.rdocumentation.org/packages/mediation/versions/4.5.0/topics/mediate) warns us that (emphasis added):

> As of version 4.0, the mediator model can be of either 'lm', 'glm' (or \`bayesglm'), 'polr' (or \`bayespolr'), 'gam', 'rq', \`survreg', or \`merMod' class, corresponding respectively to the linear regression models, generalized linear models, ordered response models, generalized additive models, quantile regression models, parametric duration models, or multilevel models.. For binary response models, the 'mediator' must be a numeric variable with values 0 or 1 as opposed to a factor. Quasi-likelihood-based inferences are not allowed for the mediator model because the functional form must be exactly specified for the estimation algorithm to work. **The 'binomial' family can only be used for binary response mediators and cannot be used for multiple-trial responses**. This is due to conflicts between how the latter type of models are implemented in [**`glm`**](https://www.rdocumentation.org/link/glm?package=mediation&version=4.5.0) and how 'mediate' is currently written.

Thus, until the package author clarifies whether this implementation results in correct estimates, we are cautious about trusting them.

### Sanity check: comparing results with linear probability models

As a sanity check, to become a little more confident that `mediate` is working as intended even though it's not supposed to work well with a multiple-trial binomial mediator, we compare the results of our model that did run (`med_2.1`) with a mediation run on linear probability models (basically, using `lm()` instead of `glm()` even though the outcomes are proportions.

```{r unweighted.mediation.ideal.sanity.check}
model_m_2.2 <- lm(
  formula = informal_support_prop ~ race,
  data = dat
)

model_y_2.2 <- lm(
  formula = IPV_prop ~ race + informal_support_prop,
  data = dat
)

med_2.2 <- mediation::mediate(
  model.m = model_m_2.2, # lm
  model.y = model_y_2.2, # lm
  sims = n_sims,
  treat = "race",
  mediator = "informal_support_prop"
)

summary(med_2.2)
```

The results seem fairly similar, but this provides limited comfort.

### Bonus finding: you can specify a mediator that is not technically the variable that your mediator model predicts

Note that in the models used for `med_2.1` there is a potential discrepancy. The mediator model is formulated as `formula = cbind(informal_support, informal_support_max - informal_support) ~ race`, while the outcome model is formulated as `formula = cbind(IPV, IPV_max - IPV) ~ race + informal_support_prop`. So the outcome model is using `informal_support_prop` (our intended mediator) as a predictor of IPV, but the mediator model uses the variables `informal_support` and `informal_support_max` to internally generate the proportion of successes. In the call to `mediation::mediate()` we can then specify `mediator = "informal_support_prop"`, and all seems to work as intended. What if we used `informal_support` as predictor in the outcome model instead? We can then specify `mediator = "informal_support"` and end up with something like this:

```{r unweighted.mediation.ideal.bonus}
model_m_2.3 <- glm(
  formula = cbind(informal_support, informal_support_max - informal_support) ~ race,
  data = dat,
  family = "binomial"
)

model_y_2.3 <- glm(
  formula = cbind(IPV, IPV_max - IPV) ~ race + informal_support,
  data = dat,
  family = "binomial"
)

med_2.3 <- mediation::mediate(
  model.m = model_m_2.3, # binomial glm, with cbind() approach
  model.y = model_y_2.3, # binomial glm, with cbind() approach
  sims = n_sims,
  treat = "race",
  mediator = "informal_support"
)

summary(med_2.3)
```

Looking at the last three rows (Average Causal Mediated Effect (average), Average Direct Effect (average), and Proportion Mediated (average)), the results seem different than the ones estimated in `med_2.1` (which used `informal_support_prop` as mediator) and `med.2.2` (which used linear proability models). But looking closer, you'll notice that the estimates related to the mediator, namely of the Average Causal Mediated Effect and the Proportion Mediated just seem to be divided by three!

My assumption about how that came to be: When we use `informal_support_prop`, which can only take values on the $(0, 1)$ interval, the ACME estimates correspond to the change in probability of IPV associated with going from 0 to 1 on the mediator and thus span the full range of the mediator. When we use `informal_support`, which takes values in the set $\{0, 1, 2, 3\}$, the estimates also correspond to the increase in probability of IPV associated with an increase of 1 on the mediator (e.g., from 0 to 1), but this is only $\frac{1}{3}$ of the total possible way. Thus, applying this effect three times would be equivalent to moving from 0 to 3 on the mediator scale, the same as moving from 0 to 1 when the mediator is a proportion.[^4]

[^4]: Of course, the effects mentioned here are not really the effects of moving from X to Y on the mediator, but the mediated effect that changing from Race = "White" to Race = "Black" has on the probability of IPV *through its effect on the mediator*.

## Issue 3: Using a binary mediator and a binomial outcome --- works with cbind(successes, failures) but not with proportion as outcome and weights argument specified {#issue-3-using-a-binary-mediator-and-a-binomial-outcome-works-with-cbindsuccesses-failures-but-not-with-proportion-as-outcome-and-weights-argument-specified}

Next, we attempted to run everything with the same binomial outcome and a binary mediator.

```{r unweighted.binary.mediator.fail}
#| error: true

model_m_3 <- glm(
  formula = informal_support_binary ~ race,
  data = dat,
  family = "binomial"
)

model_y_3 <- glm(
  formula = IPV_prop ~ race + informal_support_binary,
  weights = IPV_max,
  data = dat,
  family = "binomial"
)

med_3 <- mediation::mediate(
  model.m = model_m_3, # binary glm
  model.y = model_y_3, # binomial glm, with weights argument
  sims = n_sims,
  treat = "race",
  mediator = "informal_support_binary"
)
```

Once again we find that `mediate()` complains about different weights on mediator and outcome models, even though, according to the documentation, it works with a binary mediator and a binomial outcome. We then try using the vector of successes and failures specification for the outcome model:

```{r unweighted.binary.mediator.success}
model_y_3.1 <- glm(
  formula = cbind(IPV, IPV_max - IPV) ~ race + informal_support_binary,
  data = dat,
  family = "binomial"
)

med_3.1 <- mediation::mediate(
  model.m = model_m_3,   # binary glm 
  model.y = model_y_3.1, # binomial glm, with cbind() approach
  sims = n_sims,
  treat = "race",
  mediator = "informal_support_binary"
)

summary(med_3.1)
```

This approach runs, but we are still left wondering whether we can trust the estimates.

## Issue 4: Ordinal mediator and binomial outcome don't work together {#issue-4-ordinal-mediator-and-binomial-outcome-dont-work-together}

Because we would prefer not to binarize the social support variable in order to keep all the information that we can, we also explored using an ordinal mediator model. `mediate()` works with ordinal models fit with `MASS::polr()`, so we attempted that.

```{r unweighted.mediation.polr}
#| error: true

model_m_4 <- MASS::polr(
  formula = informal_support_ordinal ~ race,
  data = dat,
  Hess = TRUE
)

model_y_4 <- glm(
  formula = cbind(IPV, IPV_max - IPV) ~ race + informal_support_ordinal,
  data = dat,
  family = "binomial"
)

med_4 <- mediation::mediate(
  model.m = model_m_4, # ordinal polr
  model.y = model_y_4, # binomial glm, with cbind() approach
  sims = n_sims,
  treat = "race",
  mediator = "informal_support_ordinal"
)
```

The first issue arises, but it's easy to fix. Again, from the [documentation of the mediate function](https://rdrr.io/cran/mediation/man/mediate.html):

> The quasi-Bayesian approximation (King et al. 2000) cannot be used if 'model.m' is of class 'rq' or 'gam', or if 'model.y' is of class 'gam', 'polr' or 'bayespolr'. In these cases, either an error message is returned or use of the nonparametric bootstrap is forced. Users should note that use of the nonparametric bootstrap often requires significant computing time, especially when 'sims' is set to a large value.

We can get around this by setting the argument `boot` to `TRUE`.

```{r unweighted.mediation.polr.boot}
#| error: true

med_4 <- mediation::mediate(
  model.m = model_m_4, # ordinal polr
  model.y = model_y_4, # binomial glm, with cbind() approach
  sims = n_sims,
  treat = "race",
  mediator = "informal_support_ordinal",
  boot = TRUE
)
```

Now we get a different problem. It seems the `boot` package is having trouble with the way we set up our outcome model --- hopefully using the other implementation will take care of that.

```{r unweighted.mediation.polr.fail}
#| error: true

model_y_4.1 <- glm(
  formula = IPV_prop ~ race + informal_support_ordinal,
  weights = IPV_max,
  data = dat,
  family = "binomial"
)

med_4 <- mediation::mediate(
  model.m = model_m_4,   # ordinal polr
  model.y = model_y_4.1, # binomial glm, with weights argument
  sims = n_sims,
  treat = "race",
  mediator = "informal_support_ordinal",
  boot = TRUE
)
```

Finally, we reach a dead-end. It seems once again the `mediate()` function is taking the prior weights we fed into our `glm()` as sampling weights and complaining that our ordinal model doesn't have the same sampling weights. If we are willing to throw away some information, we can make it all work with a binary outcome, thusly:

```{r unweighted.mediation.polr.binary}
model_y_4.2 <- glm(
  formula = IPV_binary ~ race + informal_support_ordinal,
  data = dat,
  family = "binomial"
)

med_4 <- mediation::mediate(
  model.m = model_m_4,   # ordinal polr
  model.y = model_y_4.2, # binary glm
  sims = n_sims,
  treat = "race",
  mediator = "informal_support_ordinal",
  boot = TRUE
)

summary(med_4)
```

This runs, although the bootstrap adds significantly to computing time and we had to binarize IPV to achieve it.

## Issue 5: mediate() is not compatible with svyolr() models {#issue-5-mediate-is-not-compatible-with-svyolr-models}

With this partial success, we checked to see if we could also run this latest mediation on the weighted data.

```{r weighted.olr.fail}
#| error: true

model_m_5 <- survey::svyolr(
  formula = informal_support_ordinal ~ race,
  design = dat_weights
)
```

This throws an error, which [this Stack Overflow post](https://stackoverflow.com/questions/28916377/r-error-with-polr-initial-value-in-vmmin-is-not-finite?rq=2) can help us solve. We can help the function by providing plausible start values for the coefficients and the intercepts/thresholds. We have one coefficient and three thresholds (from 0 to 1, from 1 to 2, and from 2 to 3), and we know they should be relatively close to zero as well as that the thresholds should be in ascending order. With the syntax `start = c(coefficients, thresholds)` we can make it work:

```{r weighted.olr.success}
model_m_5 <- survey::svyolr(
  formula = informal_support_ordinal ~ race,
  design = dat_weights,
  start = c(0, -1, 0, 1)
)

summary(model_m_5)
```

Ok, moving on.

```{r weighted.mediation.olr.fail}
#| error: true

model_y_5 <- survey::svyglm(
  formula = IPV_binary ~ race + informal_support_ordinal,
  design = dat_weights,
  family = "binomial"
)

med_5 <- mediation::mediate(
  model.m = model_m_5, # ordinal svyolr
  model.y = model_y_5, # binomial svyglm, with weights argument
  sims = n_sims,
  treat = "race",
  mediator = "informal_support_ordinal",
  boot = TRUE
)
```

Not this again üò≠. We knew that the weights used for the `svyglm()` and `svyolr()` models were the same, so there must be something else happening in the background. Vasco did some investigating and again [turned to Stack Overflow](https://stackoverflow.com/questions/76900094/inconsistent-weights-with-lumleys-survey-package-svyglm-vs-svyolr-error-or-b). We can see what weights are stored in the model objects generated by `svyglm()` and `svyolr()` using the `model.frame()` function. In the following two code blocks we also use `dplyr::slice_head()` to display just the first line of results output by `model.frame().`

```{r svyolr.svyglm.weights}
model.frame(model_m_5) |> dplyr::slice_head() |> gt::gt()

model.frame(model_y_5) |> dplyr::slice_head() |> gt::gt()
```

The first weight for our ordinal model appears to be 41.13614, while the binomial model uses 0.1142592. We can extract all the weights for both models and compare them. Let's look at their ratio.

```{r weight.comparison}
weight_comparison <- tibble::tibble(
  binomial_weights = model.frame(model_y_5)$`(weights)`,
  ordinal_weights = model.frame(model_m_5)$`(weights)`,
  weight_quotient = binomial_weights / ordinal_weights
)

weight_comparison |> dplyr::slice_head(n = 5) |> gt::gt()
```

Would you look at that! The ratio seems to be constant. We can check the entire `weight_quotient` column just to be extra sure:

```{r weight.quotient.column}
weight_comparison |> dplyr::count(weight_quotient) |> gt::gt()
```

There seem to be three different values that all look the same --- perhaps this will make more sense once we learn more about floating point arithmetic, but for now we'll say the ratio between the weights is always the same. On Stack Overflow, Thomas Lumley, maintainer of the `survey` package, explains in a [reply to my post](https://stackoverflow.com/a/76902559):

> Some functions in the survey package rescale the weights to have unit mean because the large weights (thousands or tens of thousands) in some national surveys can cause convergence problems.
>
> Since the results aren't affected at all, the `mediate` package could probably work around this fairly easily. However, there's a `rescale=FALSE` option to `svyglm` to not rescale the weights, provided for this sort of purpose.
>
> If you then have convergence problems in `svyglm` you could manually rescale the weights to have unit mean before doing any of the analyses.

Good to know!

Let's set `rescale = FALSE` in the `svyglm()` model and try our luck:

```{r weighted.svyglm.rescaleF.fail}
#| error: true
model_y_5.1 <- survey::svyglm(
  formula = IPV_binary ~ race + informal_support_ordinal,
  design = dat_weights,
  family = "binomial",
  rescale = FALSE
)
```

This is perplexing, but [someone else on Stack Overflow](https://stackoverflow.com/questions/66465401/error-with-svyglm-function-in-survey-package-in-r-all-variables-must-be-in-des) a couple of years ago encountered a similar problem, where object ".survey.prob.weights" was not found when `rescale` was set to `FALSE`. After looking at the [source code](https://rdrr.io/rforge/survey/src/R/surveyrep.R) and the [accepted answer by user xilliam](https://stackoverflow.com/a/66474985/12296038) to the Stack Overflow post, we try to assign the prior weights contained in the survey design object to an object named `pwts` in the global environment, and that makes `svyglm()` happy again.

```{r weighted.svyglm.rescaleF.success}
pwts <- dat_weights$pweights

model_y_5.1 <- survey::svyglm(
  formula = IPV_binary ~ race + informal_support_ordinal,
  design = dat_weights,
  family = "binomial",
  rescale = FALSE
)

# remove pwts from the global environment
rm(list = "pwts")
```

However, we can't be sure if this correctly solves the problem or introduces new errors. Vasco [posted about this issue on Stack Overflow](https://stackoverflow.com/questions/77697864/error-in-survey-package-svyglm-object-pwts-not-found-is-there-a-fix) in the hopes that perhaps Thomas Lumley can clarify this for us.

Now we try to run the mediation.

```{r weighted.mediation.svyolr.fail}
#| error: true

med_5 <- mediation::mediate(
  model.m = model_m_5,   # ordinal svyolr
  model.y = model_y_5.1, # binary svyglm
  sims = n_sims,
  treat = "race",
  mediator = "informal_support_ordinal",
  boot = TRUE
)
```

Somehow it is not working again. What if the outcome model was ordinal as well, just for fun?

```{r weighted.mediation.svyolr.fail2}
#| error: true

model_y_5.2 <- survey::svyolr(
  formula = IPV_ordinal ~ race + informal_support_ordinal,
  design = dat_weights,
  start = c(0, 0, 0, 0, seq(from = -1, to = 1, length.out = 24))
)

med_5 <- mediation::mediate(
  model.m = model_m_5,   # ordinal svyolr
  model.y = model_y_5.2, # ordinal svyolr
  sims = n_sims,
  treat = "race",
  mediator = "informal_support_ordinal",
  boot = TRUE
)
```

Ok, we give up here.

## Issue 6: Options and choices to boot --- To robustSE or not to robustSE? {#issue-6-options-and-choices-to-boot-to-robustse-or-not-to-robustse}

IDEA: Make function that retrieves some quantities (like total effect + ci + p-value) from the mediation models as well as the boot and robustSE options and outputs a table that makes it easy to see the differences. helper function get_results() can be looped over however many models there are.

For now, we've ignored an optional setting to `mediate()`: the `robustSE` argument defaults to `FALSE`, and, according to [the documentation](https://www.rdocumentation.org/packages/mediation/versions/4.5.0/topics/mediate), "If 'TRUE', heteroskedasticity-consistent standard errors will be used in quasi-Bayesian simulations. Ignored if 'boot' is 'TRUE' or neither 'model.m' nor 'model.y' has a method for **`vcovHC`** in the **`sandwich`** package." So we have three options:

1.  `boot = TRUE` and, automatically, `robustSE = FALSE`
2.  `boot = FALSE` and `robustSE = TRUE`, and
3.  `boot = FALSE` and `robustSE = FALSE`.

And we know that, when chosen, the heteroskedasticity-consistent standard errors will be calculated with the help of the `sandwich` package.

According to the `mediation` package [vignettes](https://cran.r-project.org/web/packages/mediation/vignettes/mediation.pdf), the authors recommend setting `boot = TRUE`: "In general, as long as computing power is not an issue, analysts should estimate confidence intervals via the bootstrap with more than 1000 resamples, which is the default number of simulations." So we could do that. Unfortunately, computing power *is* an issue, as we wanted to run a multiverse analysis that meant we would use the `mediate()` function a bunch of times. To figure out whether to set `robustSE` to `TRUE` or `FALSE`, it could be useful to see which option gives use results that are closer to the bootstrap option. Let's use a binary mediator and outcome, just so we are sure there aren't other issues at play.

For convenience, we'll create a function called `compare_meds()` that takes in different mediation models and outputs a table.

```{r function.mediation.results}
get_meds <- function(med_model){
  
  boot <- as.character(med_model[["boot"]])
  
  robustSE <- as.character(med_model[["robustSE"]])
  
  results_te <- tibble::tibble(
    effect = "total_effect",
    estimate = med_model[["tau.coef"]],
    ci.low = med_model[["tau.ci"]][[1]],
    ci.high = med_model[["tau.ci"]][[2]],
    p.value = med_model[["tau.p"]]
  )
  
  results_de <- tibble::tibble(
    effect = "direct_effect",
    estimate = med_model[["z.avg"]],
    ci.low = med_model[["z.avg.ci"]][[1]],
    ci.high = med_model[["z.avg.ci"]][[2]],
    p.value = med_model[["z.avg.p"]]
  )
  
  results_ie <- tibble::tibble(
    effect = "indirect_effect",
    estimate = med_model[["d.avg"]],
    ci.low = med_model[["d.avg.ci"]][[1]],
    ci.high = med_model[["d.avg.ci"]][[2]],
    p.value = med_model[["d.avg.p"]]
  )
  
  results <- dplyr::bind_rows(results_te, results_de, results_ie)
  
  output <- results |> 
    dplyr::mutate(boot = boot, robustSE = robustSE) |> 
    dplyr::relocate(boot, robustSE)
  
  return(output)
}

compare_meds <- function(med_models, effect){
  if (class(med_models) != "list") {
    stop("Argument should be a list of models")
  }
  
  med_models_df <- med_models  |> 
    purrr::map(.f = ~ get_meds(.x)) |> 
    purrr::list_rbind() |> 
    dplyr::filter(effect == !!effect)
  
  return(med_models_df)
}
```

First, we compare unweighted models. To reduce simulation-dependent variability, we set `sims = 3000`.

```{r glm.comparison}
model_m_6 <- glm(
  formula = informal_support_binary ~ race,
  data = dat,
  family = "binomial"
)

model_y_6 <- glm(
  formula = IPV_binary ~ race + informal_support_binary,
  data = dat,
  family = "binomial"
)

med_6.1 <- mediation::mediate(
  model.m = model_m_6, # binary glm
  model.y = model_y_6, # binary glm
  sims = 3000,
  boot = TRUE,
  treat = "race",
  mediator = "informal_support_binary"
)

med_6.2 <- mediation::mediate(
  model.m = model_m_6, # binary glm
  model.y = model_y_6, # binary glm
  sims = 3000,
  robustSE = TRUE,
  treat = "race",
  mediator = "informal_support_binary"
)

med_6.3 <- mediation::mediate(
  model.m = model_m_6, # binary glm
  model.y = model_y_6, # binary glm
  sims = 3000,
  robustSE = FALSE,
  treat = "race",
  mediator = "informal_support_binary"
)

mod_list <- list(med_6.1, med_6.2, med_6.3)

compare_meds(mod_list, effect = "total_effect") |> gt::gt()
compare_meds(mod_list, effect = "direct_effect") |> gt::gt()
compare_meds(mod_list, effect = "indirect_effect") |> gt::gt()
```

We can see that the results are very similar, and in fact running the mediations several times reveals that the difference seems to be due to simulation variability. Given this, Vasco would personally rather specify `robustSE = FALSE`, since it does not make sense to correct for heteroskedasticity in a logistic regression model which *cannot suffer from* heteroskedasticity in the first place.

What about if we use weighted models?

```{r svyglm.comparison}

model_m_6.1 <- survey::svyglm(
  formula = informal_support_binary ~ race,
  design = dat_weights,
  family = "binomial"
)

model_y_6.1 <- survey::svyglm(
  formula = IPV_binary ~ race + informal_support_binary,
  design = dat_weights,
  family = "binomial"
)

med_6.4 <- mediation::mediate(
  model.m = model_m_6.1, # binary svyglm
  model.y = model_y_6.1, # binary svyglm
  sims = 3000,
  boot = TRUE,
  treat = "race",
  mediator = "informal_support_binary"
)

med_6.5 <- mediation::mediate(
  model.m = model_m_6.1, # binary svyglm
  model.y = model_y_6.1, # binary svyglm
  sims = 3000,
  robustSE = TRUE,
  treat = "race",
  mediator = "informal_support_binary"
)

med_6.6 <- mediation::mediate(
  model.m = model_m_6.1, # binary svyglm
  model.y = model_y_6.1, # binary svyglm
  sims = 3000,
  robustSE = FALSE,
  treat = "race",
  mediator = "informal_support_binary"
)

mod_list <- list(med_6.4, med_6.5, med_6.6)

compare_meds(mod_list, effect = "total_effect") |> gt::gt()
compare_meds(mod_list, effect = "direct_effect") |> gt::gt()
compare_meds(mod_list, effect = "indirect_effect") |> gt::gt()
```

While the estimates are similar, the confidence intervals are *much* tighter when `boot` or `robustSE` are set to `TRUE`, and suddenly all estimates are significant! What gives?

First, an [answer by Thomas Lumley](https://stackoverflow.com/a/75128332/12296038) (maintainer of the `survey` package) on Stack Overflow suggests that bootstrapping doesn't work well with these survey-weighted models, although his answer does not fully map to our situation. In [another answer](https://stackoverflow.com/a/56317510/12296038), this time about applying robust standard errors with the `sandwich` package[^5], Lumley is clearer: "For `svyglm` models, `vcov()` already produces the appropriate sandwich estimator, and I don't think the"sandwich" package knows enough about the internals of the object to get it right." A few years prior, Achim Zeileis, maintainer of the `sandwich` package, had given [a different perspective](https://stackoverflow.com/a/28840854/12296038) which also boils down to **do not use `sandwich`** **on models fit with `survey`**.

[^5]: Which Lumley apparently has also worked on!

There you have it, `boot = FALSE` and `robustSE = FALSE` is the way to go.

### Bonus: this goes for using `marginaleffects` as well

Often we are not interested in mediation, but rather would like a clean and interpretable way to report results from our GLMs. One could not be faulted for preferring to do this on the predicted probabilities scale rather than dealing with odds-ratios and other headaches. For this, the `marginaleffects` package can be of enormous help.

For example, we can examine the predicted probability of experiencing IPV for Black and White women:

```{r}
marginaleffects::avg_predictions(
  model = model_y_6.1, # binary svyglm
  variables = "race"
) |> gt::gt()
```

And then compare the probabilities themselves, for which we again have the option of "correcting" the standard errors via the `vcov` argument. Look at the difference in the confidence intervals and p-values spit out when we don't or do specify `vcov = "HC"`:

```{r}
marginaleffects::avg_comparisons(
  model = model_y_6.1, # binary svyglm
  variables = "race"
) |> gt::gt()

marginaleffects::avg_comparisons(
  model = model_y_6.1, # binary svyglm
  variables = "race",
  vcov = "HC"
) |> gt::gt()
```

The estimates are the same, but the standard error (wrongly) becomes minuscule if we apply the "correction" for heteroskedasticity without realizing that `sandwich` does not work properly on models fit with `survey`.

## Issue 7: Boundaries of the confidence intervals are rounded differently

Did you notice? Each time we use `summary()` on a `mediate()` object, the lower bound of the 95% CI is reported with 5 decimal places, while the upper bound is reported with 2 decimal places. This also bothered Stack Overflow user sateayam over 4 years ago, and luckily user Roland [answered with a fix](https://stackoverflow.com/a/53852254):

```{r}
trace(mediation:::print.summary.mediate, 
      at = 11,
      tracer = quote({
        printCoefmat <- function(x, digits) {
          p <- x[, 4]
          x[, 1:3] <- sprintf("%.6f", x[, 1:3])
          x[, 4] <- sprintf("%.2f", p)
          print(x, quote = FALSE, right = TRUE)
        } 
      }),
      print = FALSE)

mediation:::print.summary.mediate(summary(med_6.6))

untrace(mediation:::print.summary.mediate)
```

Unfortunately, this solution also changes the way p-values are presented. Look again at the results from `med_6.5` and compare with the following:

```{r}
trace(mediation:::print.summary.mediate, 
      at = 11,
      tracer = quote({
        printCoefmat <- function(x, digits) {
          p <- x[, 4]
          x[, 1:3] <- sprintf("%.6f", x[, 1:3])
          x[, 4] <- sprintf("%.2f", p)
          print(x, quote = FALSE, right = TRUE)
        } 
      }),
      print = FALSE)

mediation:::print.summary.mediate(summary(med_6.5))

untrace(mediation:::print.summary.mediate)
```

The interim solution is to print the summary twice, once with the original function for expected p-value behavior, and another time with the edited function for confidence-interval consistency. Someone more well-versed might want to take a shot at adapting the code so that the behavior is maintained for the p-values[^6] and fixed for the confidence intervals.

[^6]: Perhaps taking into account user2554330's [insight in another answer](https://stackoverflow.com/a/64830084) about how p-values are stored and printed.

# Conclusion

Mediation analysis is *always* tricky business[^7], but it feels especially discouraging when the software you need doesn't seem to want to cooperate. We hope this post will at least shorten someone else's journey into getting their `mediation` and/or `survey` models to behave.

[^7]: See, e.g., [That's a Lot to Process! Pitfalls of Popular Path Models](https://journals.sagepub.com/doi/10.1177/25152459221095827) by Rohrer and colleagues.

# Session Info

```{r}
sessionInfo()
```
